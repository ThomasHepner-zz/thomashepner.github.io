<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
<generator uri="http://jekyllrb.com" version="3.0.3">Jekyll</generator>
<link href="http://thomashepner.github.io/feed.xml" rel="self" type="application/atom+xml" />
<link href="http://thomashepner.github.io/" rel="alternate" type="text/html" />
<updated>2016-09-11T23:09:36-03:00</updated>
<id>http://thomashepner.github.io/</id>
<title>Thomas Hepner</title>
<subtitle>Thoughts on RecSys readings and papers.</subtitle>
<entry>
<title>Week 5 Comments: Matrix Factorization Techniques for Recommender Systems</title>
<link href="http://thomashepner.github.io/week-5-comments-matrix-factorization-techniques-for-recommender-systems/" rel="alternate" type="text/html" title="Week 5 Comments: Matrix Factorization Techniques for Recommender Systems" />
<published>2016-09-11T21:00:00-03:00</published>
<updated>2016-09-11T21:00:00-03:00</updated>
<id>http://thomashepner.github.io/week-5-comments-matrix-factorization-techniques-for-recommender-systems</id>
<content type="html" xml:base="http://thomashepner.github.io/week-5-comments-matrix-factorization-techniques-for-recommender-systems/">&lt;h2 id=&quot;papers-content&quot;&gt;Paper’s content&lt;/h2&gt;
&lt;p&gt;This article shows how matrix factorization techniques can be used to improve performance in recommender systems, allowing incorporation of implicit feedback and other sources of knowledge.&lt;/p&gt;

&lt;p&gt;In this approach, items and users are represented as vectors of factors inferred from the item rating patterns, and correspondence between items and user factors results in a recommendation. This allows incorporation of additional information, when explicit feedback is not available (most common scenario). As implicit feedback denotes the &lt;strong&gt;presence or absence of an event&lt;/strong&gt;, it can be represented as a densely filled matrix.&lt;/p&gt;

&lt;p&gt;This model is closely related with SVD (&lt;em&gt;singular value decomposition&lt;/em&gt;), a well-known method for identifying latent semantic factors in information retrieval. Still, this method requires for factoring the user-item matrix, which can be prone to errors due to the high portion of missing values (sparse matrix).&lt;/p&gt;

&lt;p&gt;In order to minimize the regularized squared error on the set of known ratings, two machine learning approaches are user: &lt;strong&gt;stochastic gradient descent&lt;/strong&gt; and &lt;strong&gt;alternating least squares&lt;/strong&gt;. The first one is easy to implement with relatively good running time, while the other is highly preferable in systems that can provide parallelization, and in those that rely on implicit data.&lt;/p&gt;

&lt;p&gt;As most systems present the “cold-start” problem, it is helpful to incorporate additional sources of information, and matrix factorization delivers a way of doing so, integrating signal sources with enhanced user representation, modifying the user factor vectors.&lt;/p&gt;

&lt;h2 id=&quot;comments-and-critics&quot;&gt;Comments and critics&lt;/h2&gt;
&lt;p&gt;I found particularly interesting that, in using ALS as the learning algorithm, the process could be parallelized, as it is a feature needed in most systems these days. Doing further research, I found that a method called coordinate descent (CDD++) can be used for parallelizing matrix factorizations, and is scalable and efficient. It has lower time complexity per iteration than ALS, ando also achieves faster and more stable convergence than stochastic gradient descent, for larger scale matrices. More importantly, it can be parallelized in multi-core and distributed environments, therefore handling large-scale datasets and variables cannot fit in a single machine’s memory. More on this approach can be found in http://www.cs.utexas.edu/~inderjit/public_papers/kais-pmf.pdf.&lt;/p&gt;

&lt;p&gt;What was more interesting was that even as the number of processors scale, CDD++ still performs way better than the other learning algorithms. For example, using 256 processors it is 40 times faster than DSGD and 20 times faster than ALS.&lt;/p&gt;
</content>
<author>
<name>thomashepner</name>
</author>
<category term="content" />
<category term="recsys" />
<summary>Paper’s contentThis article shows how matrix factorization techniques can be used to improve performance in recommender systems, allowing incorporation of implicit feedback and other sources of knowledge.</summary>
</entry>
<entry>
<title>Week 5 Comments: Hybrid Recommender Systems: Survey and Experiments</title>
<link href="http://thomashepner.github.io/week-5-comments-hybrid-recommender-systems-survey-and-experiments/" rel="alternate" type="text/html" title="Week 5 Comments: Hybrid Recommender Systems: Survey and Experiments" />
<published>2016-09-11T21:00:00-03:00</published>
<updated>2016-09-11T21:00:00-03:00</updated>
<id>http://thomashepner.github.io/week-5-comments-hybrid-recommender-systems-survey-and-experiments</id>
<content type="html" xml:base="http://thomashepner.github.io/week-5-comments-hybrid-recommender-systems-survey-and-experiments/">&lt;h2 id=&quot;papers-content&quot;&gt;Paper’s content&lt;/h2&gt;

&lt;p&gt;Hybrid recommender systems combine two or more recommendation techniques to gain better performance with fewer of the drawbacks of any individual one. It is common to see collaborative filtering combined with other approaches when seeing this kind of systems. They can be catalogued in &lt;strong&gt;weighted&lt;/strong&gt;, where the score of an item results from al the available recommendation techniques present in the system; &lt;strong&gt;switching&lt;/strong&gt;, where a certain criterion is used to switch between approaches; &lt;strong&gt;mixed&lt;/strong&gt;, used when it is practical to make a large number of recommendations simultaneously, showing recommendations from more than one technique; &lt;strong&gt;feature combination&lt;/strong&gt;, treating collaborative information as an additional feature, and then using content-based algorithms over the dataset; &lt;strong&gt;cascade&lt;/strong&gt;, involving a staged process where additional recommendation techniques are used to refine the result; &lt;strong&gt;feature augmentation&lt;/strong&gt;, where one technique is employed to produce a classification, information then used for another recommendation technique; and &lt;strong&gt;meta-level&lt;/strong&gt;, using the model generated by the first technique as input to the next one.&lt;/p&gt;

&lt;p&gt;The paper introduces a recommendation engine where they combine knowledge-based/collaborative in a cascade manner, and conduct a series of experiments with it. In analyzing the results of the experiment, the researchers prove that there is a synergy between knowledge-based and collaborative filtering. More importantly, the ramp-up problem characteristic to CF is reduced, as this technique comes second in the cascade, therefore showing benefits to the casual user, and still improving its recommendations over time.&lt;/p&gt;

&lt;h2 id=&quot;comments-and-critics&quot;&gt;Comments and critics&lt;/h2&gt;

&lt;p&gt;This investigation makes use of a similarity metric between ratings based on their characteristics, regarded as a &lt;em&gt;heuristic similarity&lt;/em&gt; approach, that does not establish a single numeric scale to which all actions are converted, but looks at the &lt;strong&gt;similarity of users on a rating by rating basis&lt;/strong&gt;. I found this very interesting, because all rating scales that I had seen prior to this established a standard numeric value where each user appreciation could be transformed into. Also, in doing so they establish assumptions that are strong, but paired with common sense.&lt;/p&gt;

&lt;p&gt;In order to use this kind of heuristic, it is important to define the semantics of the vocabulary correctly, as it is very sensitive to each value’s sign. Still, it is important to see the trade-off between the benefits using this metric, and the drawbacks of defining opposites correctly. I think it may be very hard to think of every combination and all possible comparisons between features, and it is most definitively not scalable. In my opinion, and as we can see in the experiments, a suitable choice for using this kind of heuristic is in restaurants, movies, and small e-commerces, where data is well structured and features can be easily categorized and compared.&lt;/p&gt;
</content>
<author>
<name>thomashepner</name>
</author>
<category term="content" />
<category term="recsys" />
<summary>Paper’s content</summary>
</entry>
<entry>
<title>Week 4 Comments: Content-based Recommendation Systems</title>
<link href="http://thomashepner.github.io/week-4-comments-content-based-recommendation-systems/" rel="alternate" type="text/html" title="Week 4 Comments: Content-based Recommendation Systems" />
<published>2016-09-04T16:00:00-03:00</published>
<updated>2016-09-04T16:00:00-03:00</updated>
<id>http://thomashepner.github.io/week-4-comments-content-based-recommendation-systems</id>
<content type="html" xml:base="http://thomashepner.github.io/week-4-comments-content-based-recommendation-systems/">&lt;h2 id=&quot;papers-content&quot;&gt;Paper’s content&lt;/h2&gt;
&lt;p&gt;Content-based recommender systems follow a different approach from the ones we’ve studied so far. They analyze an item’s description to identify others that would be interesting to a certain user. As a system of this kind would need to analyze thousands of items in short periods of time, in order to recommend accurately to a user, every part of the process should be as efficient as possible. Therefore, it is of interest to represent each item in a way that it makes possible to process easily.&lt;/p&gt;

&lt;p&gt;A common approach to represent unstructured text in an item’s description is to convert the free text to a structured representation. For example, each word can be an attribute of the document, with an integer value for its frequency or maybe a boolean value for appearance. There’s a technique called &lt;strong&gt;stemming&lt;/strong&gt;, where rather than using words, the &lt;strong&gt;root forms of words are stored&lt;/strong&gt;, creating a term that reflects the common meaning of similar words. A value, called &lt;strong&gt;&lt;em&gt;tf-idf&lt;/em&gt; weight&lt;/strong&gt;, is used to identify the frequency of each term in a document, relevant to a document collection. Using this value a recommender system can tell which terms are more central to the topic of the document.&lt;/p&gt;

&lt;p&gt;To save a user’s preferences, the system saves user profiles with different types of information, storing ways to retrieve items that are of his/her interest, like a history of the user’s interaction with the system, customization settings, and even &lt;strong&gt;rules to recommend other product based on history&lt;/strong&gt;. In doing so, there are several tasks related to classification learning, using implicit and explicit feedback to construct a user’s model. It is important to note that &lt;strong&gt;implicit feedback can collect a large amount of data, but with some uncertainty&lt;/strong&gt;. Finally, the learning algorithm delivers a function that provides an estimate of the probability that a user will like an unseen item.&lt;/p&gt;

&lt;p&gt;Several methods and approaches for performing content-based recommendation are explained throughout the paper. They explain how &lt;strong&gt;decision trees and rule induction&lt;/strong&gt; are used to represent an item’s description; how nearest neighbor methods compares similarity by using different functions depending on the type of data; and how relevance feedback can help improve recommendations based on previous queries performed by the user.&lt;/p&gt;

&lt;p&gt;Finally, the paper introduces probabilistic methods and naïve Bayes methods for performing text classification in a probabilistic approach. Even though naïve Bayes’ assumption of class-conditional attribute independence is violated in this context, the algorithm still performs very well, and it has been proven that this condition is not necessary for optimality.&lt;/p&gt;

&lt;h2 id=&quot;comments-and-critics&quot;&gt;Comments and critics&lt;/h2&gt;
&lt;p&gt;This investigation’s final conclusion is that &lt;strong&gt;no content-based recommender system can give good recommendations if the content does not contain enough information to distinguish items the user likes from those the user doesn’t like&lt;/strong&gt;. Good examples that contain this kind of information are movies, restaurants and television. Still, better results are found by complementing with additional information. For example, including the opinion of other users by adding data associated to the representation of the examples.&lt;/p&gt;

&lt;p&gt;What I found most interesting though was that a content-based approach can be used alongside a collaborative-filtering system to produce better recommendations. For example, a e-book store such as Amazon can use collaborative-filtering to recommend items that a user would like, but then filter them by categories using content-based algorithms. This ways it can leave out categories such as adult’s literature when recommending to a younger user.&lt;/p&gt;

&lt;p&gt;I was interested in finding out who uses the content-based approach in their systems nowadays, and found out that music streaming service &lt;strong&gt;Pandora&lt;/strong&gt; uses machine learning methods to recommend by content, as it is &lt;strong&gt;not interested in relating the interest of one user to another&lt;/strong&gt;. &lt;span class=&quot;evidence&quot;&gt;Pandora does not rely on user’s opinions: only the musical classification of a song determines whether a station will play it. The service looks for music whose content structure is similar to the taste of the user, and chooses the songs closest to that definition, with their own filtering techniques.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Pandora’s system key method is to use an efficient proximity measure to determine neighborhoods for a radio station. As the number of songs in the platform remains fairly static, the algorithm performs in polynomial time. More so, the most important factor is the time it takes to choose the next song to play from the current station, and it can be done in &lt;em&gt;O(mn)&lt;/em&gt; time, where &lt;em&gt;m&lt;/em&gt; is the number of artists/songs in the station, and &lt;em&gt;n&lt;/em&gt; is the number of attributes that characterizes a song’s structure.&lt;/p&gt;

&lt;p&gt;Applications like Pandora are a good example of how this type of recommendation is still relevant and worth studying.&lt;/p&gt;
</content>
<author>
<name>thomashepner</name>
</author>
<category term="content" />
<category term="recsys" />
<summary>Paper’s contentContent-based recommender systems follow a different approach from the ones we’ve studied so far. They analyze an item’s description to identify others that would be interesting to a certain user. As a system of this kind would need to analyze thousands of items in short periods of time, in order to recommend accurately to a user, every part of the process should be as efficient as possible. Therefore, it is of interest to represent each item in a way that it makes possible to process easily.</summary>
</entry>
<entry>
<title>Week 4 Comments: Collaborative Filtering for Social Tagging Systems: an Experiment with CiteULike</title>
<link href="http://thomashepner.github.io/week-4-comments-collaborative-filtering-for-social-tagging-systems-an-experiment-with-citeulike/" rel="alternate" type="text/html" title="Week 4 Comments: Collaborative Filtering for Social Tagging Systems: an Experiment with CiteULike" />
<published>2016-09-04T16:00:00-03:00</published>
<updated>2016-09-04T16:00:00-03:00</updated>
<id>http://thomashepner.github.io/week-4-comments-collaborative-filtering-for-social-tagging-systems-an-experiment-with-citeulike</id>
<content type="html" xml:base="http://thomashepner.github.io/week-4-comments-collaborative-filtering-for-social-tagging-systems-an-experiment-with-citeulike/">&lt;h2 id=&quot;papers-content&quot;&gt;Paper’s content&lt;/h2&gt;
&lt;p&gt;As user-contributed content is becoming more diverse in nature and quality, and traditional 5-10 point ratings are mostly unavailable, there are some new challenges for collaborative tagging systems. The loss of quality control and fine-grained ratings in these systems can be compensated by tags and explicit connections between users. Still, the paper states that there was no standard in how to take these features into account. There is some evidence, though, that social links improve personalization quality.&lt;/p&gt;

&lt;p&gt;The experiment conducted during this investigation consisted in crawling data from CiteULike: first, crawl a user’s posted articles. Then, calculate neighborhoods of users who posted the same articles and who share the same tags. Finally, use Krovetz’ algorithm to modify tags.&lt;/p&gt;

&lt;p&gt;The algorithm tested consisted of two steps: find the neighborhood of the center user, and rank the articles to be recommended. They used three different approaches: &lt;strong&gt;Classic Collaborative Filtering&lt;/strong&gt;, &lt;strong&gt;Neighbor-weighted CF&lt;/strong&gt;, and &lt;strong&gt;BM25-based Similarity&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;While testing this different methods, they discovered that, as predicted, CCF performed worse than the other algorithms, that where very close in terms of performance. The results also led to find out that the ranking order of the recommendations was very close to the optimal one.&lt;/p&gt;

&lt;p&gt;An important conclusion was that &lt;strong&gt;including the number of raters in the ranking formula is an important factor to consider in the success of these recommendations&lt;/strong&gt;, as it takes into account the weight a good rater has, compared too poor raters of a certain article. In doing so, this practice reduces the uncertainty produced by items with too few ratings.&lt;/p&gt;

&lt;p&gt;As Pearson’s correlation performs badly in this context, tag-based approaches can be a suitable alternative to obtain user neighborhood in social tagging systems.&lt;/p&gt;

&lt;h2 id=&quot;comments-and-critics&quot;&gt;Comments and critics&lt;/h2&gt;
&lt;p&gt;The paper introduces the BM25-based formula for calculating similarity, a non-binary probabilistic method used in information retrieval that returns the relevance that the documents of one collection has given a query. It compares the tags of the center user with a query, and the set of tags of each neighbor as a document. This method was first used in the Okapi BM25 information retrieval system in the 1980s.&lt;/p&gt;

&lt;p&gt;There are several extensions of the algorithm, like BM25F and BM25+. The first one considers the document to be composed from several fields, like headers and main text; the second addresses unfairly scored documents that have a similar relevance to shorter documents that do not contain the search query at all.&lt;/p&gt;

&lt;p&gt;I think the use of information retrieval techniques is a good choice in this kind of systems, especially because tags are a structured form of information, therefore making it easier to retrieve information and rank recommendations.&lt;/p&gt;
</content>
<author>
<name>thomashepner</name>
</author>
<category term="content" />
<category term="recsys" />
<summary>Paper’s contentAs user-contributed content is becoming more diverse in nature and quality, and traditional 5-10 point ratings are mostly unavailable, there are some new challenges for collaborative tagging systems. The loss of quality control and fine-grained ratings in these systems can be compensated by tags and explicit connections between users. Still, the paper states that there was no standard in how to take these features into account. There is some evidence, though, that social links improve personalization quality.</summary>
</entry>
<entry>
<title>Week 3 Comments: Recommender Systems: Sources of Knowledge and Evaluation Metrics</title>
<link href="http://thomashepner.github.io/week-3-comments-recommender-systems-sources-of-knowledge-and-evaluation-metrics/" rel="alternate" type="text/html" title="Week 3 Comments: Recommender Systems: Sources of Knowledge and Evaluation Metrics" />
<published>2016-08-28T15:35:00-03:00</published>
<updated>2016-08-28T15:35:00-03:00</updated>
<id>http://thomashepner.github.io/week-3-comments-recommender-systems-sources-of-knowledge-and-evaluation-metrics</id>
<content type="html" xml:base="http://thomashepner.github.io/week-3-comments-recommender-systems-sources-of-knowledge-and-evaluation-metrics/">&lt;h2 id=&quot;papers-content&quot;&gt;Paper’s content&lt;/h2&gt;
&lt;p&gt;This paper offers an introduction to Recommender Systems, doing a quick review of their history and some of the most popular related algorithms used. First, it shows how these systems are classified in terms of &lt;strong&gt;how they get their input information&lt;/strong&gt;, their learning process (&lt;strong&gt;memory or model based&lt;/strong&gt;), etc. Thus, they fall into three categories: &lt;strong&gt;rule-based&lt;/strong&gt;, &lt;strong&gt;content-based&lt;/strong&gt;, and &lt;strong&gt;usage-based&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Ruler-based systems make decision based on some rules extracted from user profiles. They’re popular within e-commerce systems, where administrators can set rules based on statistical, psychological and demographic information about users. A problem with this systems are the complex methods used in generating profiles, and biased inputs for the filters.&lt;/p&gt;

&lt;p&gt;Content-based recommender systems provide recommendations to users based on comparing items to others that the user has shown interest in. Thus, the user’s profile becomes an explanation of product characteristics that the user chose before. It relies on information retrieval techniques such as classification, clustering and text analysis.&lt;/p&gt;

&lt;p&gt;Finally, Collaborative Filtering systems are also successful in e-commerce solutions, performing standard memory-based classification based on neighborhoods.&lt;/p&gt;

&lt;p&gt;More importantly, the paper assesses several evaluation metrics for recommender systems. They can be classified into &lt;strong&gt;prediction based metrics&lt;/strong&gt;, that tells which algorithm makes fewer mistakes when inferring how a user will evaluate a proposed recommendation; &lt;strong&gt;information retrieval related metrics&lt;/strong&gt;, used when the user has the option to evaluate an item as relevant or not from a list of recommendations; and &lt;strong&gt;diversity, novelty, and coverage&lt;/strong&gt;, to compare between algorithms.&lt;/p&gt;

&lt;h2 id=&quot;comments-and-critics&quot;&gt;Comments and critics&lt;/h2&gt;
&lt;p&gt;As this paper is more of a review than an investigation, it is an informative way of displaying the state of the art in recommender systems (and as it was released in 2013, it is still an updated source of information).&lt;/p&gt;

&lt;p&gt;I found interesting to learn about the Netflix Prize, and how a contest could drive a whole industry into improving its performance. Researching further, I found out that the secret before team Chaos’ success was combining different algorithms in their own account to improve final performance.&lt;/p&gt;

&lt;p&gt;Several attempts of the team would lead into 7%, 8% improvements for the recommendation algorithm. Surprisingly (and un-intuitively), combining different approaches led to better results, even by continuously introducing new approaches to the solution.&lt;/p&gt;

&lt;p&gt;Some of the team’s ideas were slicing the dataset by frequency, as people who rate a big bunch of movies at once tend to be rating movies that they watched a long time ago, showing different criteria for doing so. They also discovered that people tend to rate differently on Fridays versus Mondays, so moods appear by day of the week.&lt;/p&gt;

&lt;p&gt;This relates to the paper because of the importance of knowing which evaluation metric to use in studying an algorithm. In the Netflix Prize, they used the Root Mean Squared Error metric, giving less importance to coverage, novelty and diversity, and focusing on accuracy.&lt;/p&gt;
</content>
<author>
<name>thomashepner</name>
</author>
<category term="metrics" />
<category term="sources" />
<category term="recsys" />
<summary>Paper’s contentThis paper offers an introduction to Recommender Systems, doing a quick review of their history and some of the most popular related algorithms used. First, it shows how these systems are classified in terms of how they get their input information, their learning process (memory or model based), etc. Thus, they fall into three categories: rule-based, content-based, and usage-based.</summary>
</entry>
<entry>
<title>Week 2 Comments: Slope One Predictors for Online Rating-Based Collaborative Filtering</title>
<link href="http://thomashepner.github.io/week-2-comments-slope-one-predictors-for-online-rating-based-collaborative-filtering/" rel="alternate" type="text/html" title="Week 2 Comments: Slope One Predictors for Online Rating-Based Collaborative Filtering" />
<published>2016-08-21T22:00:00-03:00</published>
<updated>2016-08-21T22:00:00-03:00</updated>
<id>http://thomashepner.github.io/week-2-comments-slope-one-predictors-for-online-rating-based-collaborative-filtering</id>
<content type="html" xml:base="http://thomashepner.github.io/week-2-comments-slope-one-predictors-for-online-rating-based-collaborative-filtering/">&lt;h2 id=&quot;papers-content&quot;&gt;Paper’s content&lt;/h2&gt;
&lt;p&gt;This research shows another model-based approach for making collaborative filtering. The researcher’s main focus is to prove that a new model, called Slope One predictors, could provide:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ease of implementation.&lt;/li&gt;
  &lt;li&gt;Ability to be updateable at execution.&lt;/li&gt;
  &lt;li&gt;Efficiency at query time: queries fast at the expense of storage.&lt;/li&gt;
  &lt;li&gt;Little expectation from first visitors.&lt;/li&gt;
  &lt;li&gt;Accuracy within reason: a minor gain in accuracy is not always worth a major sacrifice in simplicity or scalability.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A concept called &lt;em&gt;popularity differential&lt;/em&gt; is introduced, or how much better is one item liked than another. Many of this differentials exist within a training set for each unknown rating, and Slope One loosely takes advantage of them.&lt;/p&gt;

&lt;p&gt;More specifically, Slope One predictors take into account both information from users who rated the same item, and from the other items rated by the same user - but it also considers data that is not factored in. In doing this, it only takes in ratings by users &lt;strong&gt;who have rated some common item with the &lt;em&gt;predictee&lt;/em&gt; user&lt;/strong&gt;, and only those ratings of items &lt;strong&gt;that the predictee user has also rated&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;But this first approach doesn’t take into account the number of ratings an observer has. As a user who has many ratings would be a much better predictor, the &lt;strong&gt;weighted slope one predictor&lt;/strong&gt; takes it into consideration. A more specific elaboration of the algorithm, called the &lt;strong&gt;bipolar slope one scheme&lt;/strong&gt;, splits the prediction in two parts: one for users that liked and one for users that disliked an item, applying a threshold between user’s liked and disliked items.&lt;/p&gt;

&lt;h2 id=&quot;comments-and-critics&quot;&gt;Comments and critics&lt;/h2&gt;
&lt;p&gt;The first consideration to have while studying Slope One is the one regarding the splitting the prediction into two. This is because this action “doubles” the number of users, but reduces the overall number of ratings in the calculation of the prediction, with an improvement in accuracy. Furthermore, splitting ratings into like and dislike subsets could improve accuracy even more. So, it is important to keep in mind a very quantifiable drawback in performance while separating predictions into groups.&lt;/p&gt;

&lt;p&gt;Another question arises: what happens when the rating scale is not necessarily ordinal, when the difference between a 5-star rating and 4-stars is not the same as the one between 4-stars and 3-stars? It seems that calculating a threshold would be a more complex operation, hence making it harder to improve accuracy.&lt;/p&gt;

&lt;p&gt;Still, this algorithm finally addresses the scalability problem while making the fair assumption that you only use a “cross shaped section” of the user-item matrix. Even though this assumption, I find it smart, given that the sparseness of the matrix is also being taken care of.&lt;/p&gt;

&lt;p&gt;Further diving into recommender system’s history would show that Slope One predictors would become an important improvement, and be rapidly used by several enterprises.&lt;/p&gt;
</content>
<author>
<name>thomashepner</name>
</author>
<category term="collaborative" />
<category term="filtering" />
<category term="recsys" />
<summary>Paper’s contentThis research shows another model-based approach for making collaborative filtering. The researcher’s main focus is to prove that a new model, called Slope One predictors, could provide:</summary>
</entry>
<entry>
<title>Week 2 Comments: Item-based Collaborative Filtering Recommendation Algorithms</title>
<link href="http://thomashepner.github.io/week-2-comments-item-based-collaborative-filtering-recommendation-algorithms/" rel="alternate" type="text/html" title="Week 2 Comments: Item-based Collaborative Filtering Recommendation Algorithms" />
<published>2016-08-21T22:00:00-03:00</published>
<updated>2016-08-21T22:00:00-03:00</updated>
<id>http://thomashepner.github.io/week-2-comments-item-based-collaborative-filtering-recommendation-algorithms</id>
<content type="html" xml:base="http://thomashepner.github.io/week-2-comments-item-based-collaborative-filtering-recommendation-algorithms/">&lt;h2 id=&quot;papers-content&quot;&gt;Paper’s content&lt;/h2&gt;
&lt;p&gt;This paper introduces a brand new approach to Collaborative Filtering. Instead of computing predictions for ratings that a user would give to an item based on user-to-user similarity, this algorithm isolates the users that have rated a given pair of items an then computes the similarity &lt;strong&gt;between those items&lt;/strong&gt;. They also present several ways to calculate this similarity, like &lt;em&gt;cosine based&lt;/em&gt;, &lt;em&gt;correlation based&lt;/em&gt;, and &lt;em&gt;adjusted-cosine&lt;/em&gt; based similarities, and show how to use them to make predictions.&lt;/p&gt;

&lt;p&gt;While trying to make predictions with this approach, the problem of two distant item vector arises: correlation may be misleading when the Euclidian distance between vectors is large, but they are still similar. That’s why they introduce a regression model for weighting similarities, so instead of using raw rating values, approximated values are used based on a linear regression model.&lt;/p&gt;

&lt;p&gt;This research’s focus is on finding out if this different approach to Collaborative Filtering is similar or better than regular Collaborative Filtering, especially in terms of scalability. They discovered that, as item-based recommendation is model-based, it provided higher scalability, as it isolated neighborhood generation and prediction generation steps. This is mainly because the set of items is static compared to the number of users, that changes more often. Still, precomputing all item-to-item similarities would need O(n^2) space, which is not desirable in terms of performance. A solution to this would be retaining only small numbers of similar items (&lt;em&gt;k&lt;/em&gt; sized groups), with an obvious tradeoff in accuracy and coverage.&lt;/p&gt;

&lt;h2 id=&quot;comments-and-critics&quot;&gt;Comments and critics&lt;/h2&gt;
&lt;p&gt;I was skeptical on this new approach’s performance at first. It seemed to me than “inverting the problem’s matrix” would not yield improvements of any kind. But it turns out that the researchers have a fair point in having the strong assumption that the number or items remains fairly static through time, at least compared to the amount of users.&lt;/p&gt;

&lt;p&gt;While trying to understand this approach, I took as an example the CiberDays that several e-commerces organize every now and then. Even though they introduce a large amount of new items every season, user growth escalates way quicker, and the advantages provided by pre-computation of item similarity may give an e-commerce platform the upper hand against its competition while trying to engage more users in better ways and in limited time.&lt;/p&gt;

&lt;p&gt;Although this ways makes more sense (at least to me: when you are in an e-commerce store, you expect to see related items to what you are searching - that’s a fair assumption), it appears that the advantages are not as grand as one would expect, so the added complexity may not be worth the risk.&lt;/p&gt;

&lt;p&gt;Still, this paper shows something that would be very important in the coming years: model-based algorithms provide high online performance, while making fairly strong assumptions. So by knowing the dataset well, you could implement a solution similar to this without loosing recommendation quality.&lt;/p&gt;
</content>
<author>
<name>thomashepner</name>
</author>
<category term="collaborative" />
<category term="filtering" />
<category term="recsys" />
<summary>Paper’s contentThis paper introduces a brand new approach to Collaborative Filtering. Instead of computing predictions for ratings that a user would give to an item based on user-to-user similarity, this algorithm isolates the users that have rated a given pair of items an then computes the similarity between those items. They also present several ways to calculate this similarity, like cosine based, correlation based, and adjusted-cosine based similarities, and show how to use them to make predictions.</summary>
</entry>
<entry>
<title>Week 2 Comments: An Algorithmic Framework For Performing Collaborative Filtering</title>
<link href="http://thomashepner.github.io/week-2-comments-an-algorithmic-framework-for-performing-collaborative-filtering/" rel="alternate" type="text/html" title="Week 2 Comments: An Algorithmic Framework For Performing Collaborative Filtering" />
<published>2016-08-21T15:35:00-03:00</published>
<updated>2016-08-21T15:35:00-03:00</updated>
<id>http://thomashepner.github.io/week-2-comments-an-algorithmic-framework-for-performing-collaborative-filtering</id>
<content type="html" xml:base="http://thomashepner.github.io/week-2-comments-an-algorithmic-framework-for-performing-collaborative-filtering/">&lt;h2 id=&quot;papers-content&quot;&gt;Paper’s content&lt;/h2&gt;
&lt;p&gt;I’ll be reviewing and commenting on a paper made by the GroupLens team on year 1999, where the team tried to go a little bit further on the Collaborative Filtering topic and issue some of the problems that rise when performing related tasks. It also introduces the advantages of using this method in recommender systems, and the most important metrics used to evaluate their performance.&lt;/p&gt;

&lt;p&gt;Particularly important and relevant in this investigation is the definition of metrics for measuring a system’s performance. They assess three key dimensions: &lt;strong&gt;coverage&lt;/strong&gt; (percentage of items for which a recommender system can provide predictions), &lt;strong&gt;statistical accuracy&lt;/strong&gt; (compares the numerical prediction values against user ratings for the for the items that have both predictions and ratings), and &lt;strong&gt;decision-support accuracy&lt;/strong&gt; (how effectively predictions help a user select high-quality items from the item set).&lt;/p&gt;

&lt;p&gt;They based their experiments on different ways for weighting users with respect to different relevant dimensions. The first one is users &lt;strong&gt;similarity&lt;/strong&gt;, based on the assumption that people trust historical providers of accurate recommendations. Then they focus on &lt;strong&gt;significance&lt;/strong&gt;, addressing the amount of trust to be placed in a correlation with another user. Finally, they study &lt;strong&gt;variance&lt;/strong&gt;, when some ratings a user gives to certain items is more valuable than others in discerning his/her interest.&lt;/p&gt;

&lt;p&gt;Finally, the measured and studied algorithms for producing predictions and selecting a user’s neighborhood, both very sensitive to scalability and performance issues.&lt;/p&gt;

&lt;h2 id=&quot;comments-and-critics&quot;&gt;Comments and critics&lt;/h2&gt;
&lt;p&gt;This paper succeeds in presenting the recommendation problem as separate components, and delivering new algorithms that enhance the accuracy of predictions. I found interesting the introduction of the Spearman rank correlation coefficient, similar to Pearson’s but that doesn’t rely on strict model assumptions that may not apply to every dataset. Still, this method does not work well when ranking scales are too small, because of the amount of tied rankings.&lt;/p&gt;

&lt;p&gt;I found especially important the introduction of ROC sensitivity for measuring the diagnostic power of a filtering system. This metric compares &lt;strong&gt;sensitivity&lt;/strong&gt;, or the probability of a randomly selecting good item being accepted, against &lt;strong&gt;specificity&lt;/strong&gt;. the chance of a randomly selected bad item being rejected by the filter. It is important because it introduces the concept of &lt;em&gt;goodness&lt;/em&gt; in information filtering, and also considers the probability of type I or type II errors in the test’s hypothesis.&lt;/p&gt;

&lt;p&gt;Finally, the document still fails to address an important issue: scalability. Perhaps because large datasets of millions of user ratings didn’t exist at the time. Even though this wasn’t the research’s primary focus - they based their study in separating content filtering into separate components - I found out that the computing complexity of the problem was not addressed, and only in later related readings there were investigators trying to discover model-based solutions to the problem, as the Slope One Algorithm or Item-based content filtering.&lt;/p&gt;
</content>
<author>
<name>thomashepner</name>
</author>
<category term="collaborative" />
<category term="filtering" />
<category term="recsys" />
<summary>Paper’s contentI’ll be reviewing and commenting on a paper made by the GroupLens team on year 1999, where the team tried to go a little bit further on the Collaborative Filtering topic and issue some of the problems that rise when performing related tasks. It also introduces the advantages of using this method in recommender systems, and the most important metrics used to evaluate their performance.</summary>
</entry>
<entry>
<title>Week 1 Comments: Collaborative Filtering Introduction</title>
<link href="http://thomashepner.github.io/week-1-comments-collaborative-filtering-introduction/" rel="alternate" type="text/html" title="Week 1 Comments: Collaborative Filtering Introduction" />
<published>2016-08-14T19:35:00-03:00</published>
<updated>2016-08-14T19:35:00-03:00</updated>
<id>http://thomashepner.github.io/week-1-comments-collaborative-filtering-introduction</id>
<content type="html" xml:base="http://thomashepner.github.io/week-1-comments-collaborative-filtering-introduction/">&lt;h2 id=&quot;introduction-to-recommender-systems&quot;&gt;Introduction to Recommender Systems&lt;/h2&gt;

&lt;p&gt;Recommender systems are platforms that try to solve the &lt;strong&gt;recommendation problem&lt;/strong&gt;, in which they seek to predict rating or preference that a user would give to an item.&lt;/p&gt;

&lt;p&gt;I’ll be mainly reviewing a paper by the Grouplens Social Computing Research team at the University of Minnesota, which introduces the concept of Collaborative Filtering and builds a foundation on Recommender Systems theory.&lt;/p&gt;

&lt;h2 id=&quot;grouplens-grouplens-an-open-architecture-for-collaborative-filtering-of-netnews&quot;&gt;Grouplens: Grouplens: An Open Architecture for Collaborative Filtering of Netnews&lt;/h2&gt;

&lt;p&gt;This Grouplens paper offers evidence on the work done while trying to make recommendations for users of a news system called Usenet. Basically, a user could rate a post by scoring it from 1 (bad) to 5 (good), but the important thing about it is that the research group relied on the premise that &lt;strong&gt;a user rates an article with the score he/she thinks the platform would suggest&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;It is then introduced a simple method of rating prediction, using &lt;strong&gt;Pearson’s Correlation&lt;/strong&gt; between other users ratings, in order to predict the rating the active user would give to the un-rated article. In the examples shown in the paper, a positive value of correlation between two users means that they agree more, and a negative value suggests disagreement.&lt;/p&gt;

&lt;p&gt;The predicted rating value is then calculated combining this correlations, in order to fill a user/rating matrix an then use predictions however the systems deems necessary.&lt;/p&gt;

&lt;p&gt;Finally, the investigation sheds light on the issues natural to the recommendation problem, which will be commented further in following sections.&lt;/p&gt;

&lt;h3 id=&quot;comments-and-critics&quot;&gt;Comments and critics&lt;/h3&gt;

&lt;p&gt;First and foremost, it is interesting to see this paper as a precursor to recommender systems theory. Noting that it was written in 1994, it is impressive to see how visionary the Grouplens team was in discovering this problem and the needs for more specific information filtering. What impresses me is the fact that they were dealing with data traffic close to 100 MB per day, with user postings of over 140,000 articles in a two-week basis, which falls very short to today’s standards and needs for information.&lt;/p&gt;

&lt;p&gt;This paper can be rightfully called a precursor to recommender systems theory because of the many investigations that rely upon it. It currently has 1,172 citations in the ACM academic network.&lt;/p&gt;

&lt;p&gt;However, I found that the choice of using Pearson’s correlation was poorly explained, and in later investigations it is questioned by some authors, as the data should adapt to very strict models to give confident predictions. For example, the authors of “An Algorithmic Framework for Performing Collaborative Filtering”, five years later mentions that “Pearson’s correlation coefficient is derived from a linear regression model that relies in a set of assumptions regarding the data, namely that the relationship must be linear, and the errors must be independent and have a probability distribution with mean 0 an constant variance for every setting of the independent variable”. It is common to see these assumptions violated, so it is important to understand to which datasets the model adjusts better.&lt;/p&gt;

&lt;p&gt;Still, the paper succeeds in determining the scaling issues that these platforms have, and in showing some ways in which they can be tackled. They assessed score prediction’s quality, rating’s posting and prediction’s reception waiting time, and network traffic issues.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Resnick, P., Iacovou, N., Suchak, M., Bergstrom, P., &amp;amp; Riedl, J. (1994, October). GroupLens: an open architecture for collaborative filtering of netnews. In Proceedings of the 1994 ACM conference on Computer supported cooperative work (pp. 175-186). ACM.&lt;/li&gt;
  &lt;li&gt;Herlocker, J. L., Konstan, J. A., Borchers, A., &amp;amp; Riedl, J. (1999, August). An algorithmic framework for performing collaborative filtering. In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval (pp. 230-237).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;evan-millers-blog-how-not-to-sort-by-average-rating&quot;&gt;Evan Miller’s blog: How Not To Sort By Average Rating&lt;/h2&gt;
&lt;p&gt;This post was interesting as it proposes a simple way for sorting rated items, using the lower bound of Wilson score confidence interval for a Bernoulli parameter. The good part is that the post suggest a couple of ways for implementing the score assignment algorithm, including Excel, Ruby and SQL statements.&lt;/p&gt;

&lt;h3 id=&quot;references-1&quot;&gt;References&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.evanmiller.org/how-not-to-sort-by-average-rating.html&quot;&gt;http://www.evanmiller.org/how-not-to-sort-by-average-rating.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
<author>
<name>thomashepner</name>
</author>
<category term="collaborative" />
<category term="filtering" />
<category term="recsys" />
<summary>Introduction to Recommender Systems</summary>
</entry>
</feed>
