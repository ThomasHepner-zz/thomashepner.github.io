<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
<generator uri="http://jekyllrb.com" version="3.0.3">Jekyll</generator>
<link href="http://thomashepner.github.io/feed.xml" rel="self" type="application/atom+xml" />
<link href="http://thomashepner.github.io/" rel="alternate" type="text/html" />
<updated>2016-11-06T22:34:56-03:00</updated>
<id>http://thomashepner.github.io/</id>
<title>Thomas Hepner</title>
<subtitle>Thoughts on RecSys readings and papers.</subtitle>
<entry>
<title>Week 13 Comments: A new approach to evaluating novel recommendations.</title>
<link href="http://thomashepner.github.io/week-13-comments-a-new-approach-to-evaluating-novel-recommendations/" rel="alternate" type="text/html" title="Week 13 Comments: A new approach to evaluating novel recommendations." />
<published>2016-11-06T21:00:00-03:00</published>
<updated>2016-11-06T21:00:00-03:00</updated>
<id>http://thomashepner.github.io/week-13-comments-a-new-approach-to-evaluating-novel-recommendations</id>
<content type="html" xml:base="http://thomashepner.github.io/week-13-comments-a-new-approach-to-evaluating-novel-recommendations/">&lt;h2 id=&quot;papers-content&quot;&gt;Paper’s content&lt;/h2&gt;

&lt;h2 id=&quot;comments-and-critics&quot;&gt;Comments and critics&lt;/h2&gt;
</content>
<author>
<name>thomashepner</name>
</author>
<category term="algorithms" />
<category term="novel" />
<summary>Paper’s content</summary>
</entry>
<entry>
<title>Week 12 Comments: The link prediction problem for social networks</title>
<link href="http://thomashepner.github.io/week-12-comments-the-link-prediction-problem-for-social-networks/" rel="alternate" type="text/html" title="Week 12 Comments: The link prediction problem for social networks" />
<published>2016-10-30T21:00:00-03:00</published>
<updated>2016-10-30T21:00:00-03:00</updated>
<id>http://thomashepner.github.io/week-12-comments-the-link-prediction-problem-for-social-networks</id>
<content type="html" xml:base="http://thomashepner.github.io/week-12-comments-the-link-prediction-problem-for-social-networks/">&lt;h2 id=&quot;papers-content&quot;&gt;Paper’s content&lt;/h2&gt;
&lt;p&gt;This paper introduces the problem of link prediction in recommender systems: given a snapshot of a social network in a given time, it seeks to accurately predict the edges that will be added to the network during the interval from the present to some specific future time. This can be applied to many situations in social networks; the example used in the research is based on a graph of collaborations between scientists, but a most familiar example could be to predict future friendships in a social network such as Facebook. More specifically, this investigation seeks to answer the question of to what extent do the intrinsic features of a social network provide useful information to model its evolution. Even though sources of new relationships occur independent to the graph’s topology, the authors state that a large number of new collaborations are hinted by the topology of the network. This could serve for many applications, such as security (conjectures that particular individuals are working together even though their interaction has not been directly observed), or to infer missing links from an observed network.&lt;/p&gt;

&lt;p&gt;Several methods for link prediction are introduced, where they assign a connection weight score to pairs of nodes based on the input graph, and then produce a ranked list in decreasing order of score. The most basic approach was to rank pairs of nodes based on the length of their shortest path in the graph. More elaborate methods included using node neighborhoods, implicitly considering the ensemble of all paths between two nodes, and adding meta-approaches to clean the graph of tenuous edges.&lt;/p&gt;

&lt;p&gt;As it was said before, many collaborations form for reasons outside the scope of the network, which means that the raw performance of the paper’s predictors is relatively low. Still, comparing with a random predictor suggested that there is indeed useful information contained in the topology of the graph alone. But this also leads to the small world problem, which consists of graphs full with short paths between nodes and makes the link prediction problem less approachable.&lt;/p&gt;

&lt;h2 id=&quot;comments-and-critics&quot;&gt;Comments and critics&lt;/h2&gt;
&lt;p&gt;I found this investigation quite interesting and easy to follow, but two questions lingered on after reading. First, the authors mentioned that they identified the authors of their dataset by their initials, which led to too much noise. I found that to be poorly explained, as authors could have been identified by unique IDs without introducing noise, but I suppose that there’s a reason for not doing that, and the authors just didn’t explain its complications. Also, when defining the &lt;em&gt;Core&lt;/em&gt; used for testing each method, they justified that definition because the experiments would not be sensitive to the Core’s size, but without showing how the size of the Core and the results of the experiment are independent.&lt;/p&gt;

&lt;p&gt;The other question rose when reading about overlaps in several predictors. Unsurprisingly, many predictors have common predictions, but some of the large overlaps do not seem to follow obviously from the predictions of the measures. The authors justification for this statement is that some datasets have simpler structures than others, but don’t explain what they mean with the word “simpler”, leaving me to guess that some datasets are less dense than others, which impacts overlapping.&lt;/p&gt;

&lt;p&gt;Finally, I found that the paper refers excessively to collaborations between scientists, which leads to less understanding of applications that each method could have in other fields. For example, their propose clustering methods for cleaning the graph or for making better predictions, maybe by grouping scientists by their investigation field. But how can that be applied to a friendship context such as Facebook? Maybe a more general framework for clustering by categories could have been useful to better understand how they can be applied to exploit the graph’s topology.&lt;/p&gt;
</content>
<author>
<name>thomashepner</name>
</author>
<category term="algorithms" />
<category term="graphs" />
<category term="recsys" />
<category term="links" />
<category term="social" />
<summary>Paper’s contentThis paper introduces the problem of link prediction in recommender systems: given a snapshot of a social network in a given time, it seeks to accurately predict the edges that will be added to the network during the interval from the present to some specific future time. This can be applied to many situations in social networks; the example used in the research is based on a graph of collaborations between scientists, but a most familiar example could be to predict future friendships in a social network such as Facebook. More specifically, this investigation seeks to answer the question of to what extent do the intrinsic features of a social network provide useful information to model its evolution. Even though sources of new relationships occur independent to the graph’s topology, the authors state that a large number of new collaborations are hinted by the topology of the network. This could serve for many applications, such as security (conjectures that particular individuals are working together even though their interaction has not been directly observed), or to infer missing links from an observed network.</summary>
</entry>
<entry>
<title>Week 11 Comments:  A survey of active learning in collaborative filtering recommender systems</title>
<link href="http://thomashepner.github.io/week-11-comments-a-survey-of-active-learning-in-collaborative-filtering-recommender-systems/" rel="alternate" type="text/html" title="Week 11 Comments:  A survey of active learning in collaborative filtering recommender systems" />
<published>2016-10-23T21:00:00-03:00</published>
<updated>2016-10-23T21:00:00-03:00</updated>
<id>http://thomashepner.github.io/week-11-comments-a-survey-of-active-learning-in-collaborative-filtering-recommender-systems</id>
<content type="html" xml:base="http://thomashepner.github.io/week-11-comments-a-survey-of-active-learning-in-collaborative-filtering-recommender-systems/">&lt;h2 id=&quot;papers-content&quot;&gt;Paper’s content&lt;/h2&gt;
&lt;p&gt;This paper researches about the state of the art on active learning, or how recommender systems tackle the problem of obtaining &lt;strong&gt;high quality data&lt;/strong&gt; that better represents the user’s preferences and improves the recommendation quality, done by asking the user to provide further ratings on items not yet rated, acquiring additional data. Active learning is motivated by situations where training data is expensive to obtain (like ratings, as users are reluctant to rate items).&lt;/p&gt;

&lt;p&gt;This paper considers two active learning strategies relevant to collaborative filtering: &lt;strong&gt;personalization&lt;/strong&gt;, that describes the extent to which the selection of items for the user to rate is adapted to the user’s characteristics; and &lt;strong&gt;hybridization&lt;/strong&gt;, or whether the strategy takes only a single heuristic for selecting the items or if it combines several criteria.&lt;/p&gt;

&lt;p&gt;The authors make an introduction to recommender systems and collaborative filtering, and then explain the concept of active learning, independent from the recommendation problem context. What’s interesting, though, are active learning applications to collaborative filtering. One would think that active learning would be similar to content-based or context-aware recommendation, where the model provides more information that could be useful to make better prediction. But what it actually does is make the user-item matrix less sparse, therefore improving accuracy and overall performance. Without going into much detail, active learning tries to predict user’s ratings based on groups of recommendations that can give better predictions if selected.&lt;/p&gt;

&lt;p&gt;By separating active learning strategies into personalized and non-personalized, and there into single and combined heuristics, a large number of techniques arises, each one with metrics for evaluation.&lt;/p&gt;

&lt;h2 id=&quot;comments-and-critics&quot;&gt;Comments and critics&lt;/h2&gt;
&lt;p&gt;This investigation offers a very complete framework for selecting an active learning strategy in an operational system. For each technique, it shows its benefits and drawbacks, therefore making it easier and less riskier to implement. Finally, the authors provide a useful comparison table to better see the strategies side by side. This allows not only to see which metric is needed to evaluate each technique, but also to see if it can be done online or offline, which might be crucial for many systems.&lt;/p&gt;

&lt;p&gt;What this investigation lacks, though, is a comparison between systems that include active learning into their recommendation engines and those that don’t, because it is not clear by how much active learning improves accuracy. Also, it is not clear how much active learning improves scalability when the matrix is too sparse. The initial impression is that by doing continuous active learning, the benefits in terms of performance would be immediate. The authors state this as an open issue for investigation, and assess the problem of persuading the user to provide ratings not only on registration, but all along its use of the system. This makes sense only for a small amount of applications. For example, I don’t see it being used in applications for restaurants, where the user might not be interested in rating items when he is not actively purchasing them. Still, it depends on the problem: Yelp seems to be pretty good in engaging users into providing ratings continuously.&lt;/p&gt;

&lt;p&gt;Finally, one problem with active learning is that there is no universal agreement on which is the best algorithm to use, so an effort in studying the best option for an specific problem has to be considered if this is the approach that you want to implement.&lt;/p&gt;
</content>
<author>
<name>thomashepner</name>
</author>
<category term="algorithms" />
<category term="ux" />
<category term="recsys" />
<summary>Paper’s contentThis paper researches about the state of the art on active learning, or how recommender systems tackle the problem of obtaining high quality data that better represents the user’s preferences and improves the recommendation quality, done by asking the user to provide further ratings on items not yet rated, acquiring additional data. Active learning is motivated by situations where training data is expensive to obtain (like ratings, as users are reluctant to rate items).</summary>
</entry>
<entry>
<title>Week 10 Comments: Context-Aware Recommender Systems</title>
<link href="http://thomashepner.github.io/week-10-comments-context-aware-recommender-systems/" rel="alternate" type="text/html" title="Week 10 Comments: Context-Aware Recommender Systems" />
<published>2016-09-30T10:00:00-03:00</published>
<updated>2016-09-30T10:00:00-03:00</updated>
<id>http://thomashepner.github.io/week-10-comments-context-aware-recommender-systems</id>
<content type="html" xml:base="http://thomashepner.github.io/week-10-comments-context-aware-recommender-systems/">&lt;h2 id=&quot;papers-content&quot;&gt;Paper’s content&lt;/h2&gt;

&lt;p&gt;This investigation proposes a recommender system that also uses the context in which the rating happened to include this potentially useful information into the knowledge stream. The goal is to label every user action with an appropriate context, and then build a system that makes good use of it. Still, it is difficult to define context because of its multifaceted nature, and there are many questions to be answered in order to apply this kind of information into a recommendation. There are two views to context: &lt;strong&gt;representational&lt;/strong&gt; and &lt;strong&gt;interactional&lt;/strong&gt; contexts, where the first is independent from the underlying activity being performed, whereas the second assumes a cyclical relationship between context and activity, each one influencing the other over and over again.&lt;/p&gt;

&lt;p&gt;The approach followed by the authors makes use of &lt;strong&gt;contextual factors&lt;/strong&gt;, such as time, location, and purchasing purpose. These factors can be catalogued by the knowledge we have of them, and by how they change (static, dynamic). This two categories define a category matrix, and the authors focus only on static, fully observable context. In that manner, they define a model in which a new dimension is used to calculate ratings:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;R: Users&lt;/em&gt; x &lt;em&gt;Items&lt;/em&gt; x &lt;em&gt;Contexts&lt;/em&gt; -&amp;gt; &lt;em&gt;Ratings&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This model raises the issue of scalability right away: it needs to include a way of computing similarities efficiently, given that three dimensions would normally lead to &lt;em&gt;O(n^3)&lt;/em&gt; complexities.  In order to filter the context, three approaches are proposed: contextual prefiltering, contextual postfiltering, and contextual modeling.&lt;/p&gt;

&lt;p&gt;Contextual prefiltering uses contextual information to select the most relevant User x Item tuples for generating recommendations, where the context serves as a query for selecting the data. However, the exact context sometimes can be too narrow, and the exact context may not have enough data for accurate predictions. Later investigations assess this issue by splitting user profiles into several sub-profiles that represent the user in different problems. This, however, raises the overlapping user profiles problem.&lt;/p&gt;

&lt;p&gt;Contextual postfiltering ignores context information until a recommendation is made, and adjusts the obtained recommendation list to the context afterwards, using either a model-based or a heuristics-based approach. A major advantage of this is that it allows using any traditional recommendation technique to provide the top-N item list for a user.&lt;/p&gt;

&lt;p&gt;Contextual modeling uses context directly in the recommendation function, allowing truly multidimensional recommendation functions that may rely on decision trees, regresions, or even heuristics that incorporate contextual information. In this type of modeling it has been used a tensor based approach, in response to the popular matrix factorization method, but allowing for &lt;em&gt;n&lt;/em&gt;-dimensions in the calculations. Further research shows that context-aware SVM outperforms noncontextual SVM in terms of predictive accuracy.&lt;/p&gt;

&lt;h2 id=&quot;comments-and-critics&quot;&gt;Comments and critics&lt;/h2&gt;
&lt;p&gt;I found the paper very complete, although I missed some comparisons between noncontextual methods and CARS, specially in the POI example they provided. Still, it is interesting how they sort of “predict” the applications in which this topic would’ve involved in the future.
A couple of students from the RecSys course at PUC Chile did a comparison between implicit feedback and context-aware recommendations, where they compared Hu and Koren’s implicit feedback algorithm with tensor factorization and factorization machines in order to see tradeoffs between these approaches, using the MovieCity dataset. Even though factorization machines rating prediction outperformed the other approaches, it was interesting to find out that the implicit feedback algorithm was better in making a ranking of recommendations, therefore concluding that context not necessarily improves a top-N task.
Applied to this paper, it would seem that a combination between contextual modeling and implicit feedback could yield an optimal combination towards making recommendations to a user: using the context to make predictions, and then including implicit feedback to make a top-N recommendation. It would be interesting to see this approach in further research, building a hybrid system that uses both context and implicit feedback in the recommendation engine.&lt;/p&gt;
</content>
<author>
<name>thomashepner</name>
</author>
<category term="context" />
<category term="aware" />
<category term="recsys" />
<summary>Paper’s content</summary>
</entry>
<entry>
<title>Week 6 Comments: Collaborative Filtering for Implicit Feedback Datasets</title>
<link href="http://thomashepner.github.io/week-6-comments-collaborative-filtering-for-implicit-feedback-datasets/" rel="alternate" type="text/html" title="Week 6 Comments: Collaborative Filtering for Implicit Feedback Datasets" />
<published>2016-09-24T16:00:00-03:00</published>
<updated>2016-09-24T16:00:00-03:00</updated>
<id>http://thomashepner.github.io/week-6-comments-collaborative-filtering-for-implicit-feedback-datasets</id>
<content type="html" xml:base="http://thomashepner.github.io/week-6-comments-collaborative-filtering-for-implicit-feedback-datasets/">&lt;h2 id=&quot;papers-content&quot;&gt;Paper’s content&lt;/h2&gt;
&lt;p&gt;Historic recommender systems studies have come to distinguish two separate approaches for predicting and recommending items to a user: content-based and collaborative filtering recommendations, each with preferred applications when it comes to analyzing the dataset available or the need of the user. In this investigation, the authors introduced a new way of incorporating user data to collaborative filtering, in order to improve its accuracy, and also to assess problems such as cold start.&lt;/p&gt;

&lt;p&gt;As explicit feedback is not always available in a dataset, they proposed a way of using a much more abundant source of data: implicit feedback. This could manifest in the shape of purchase history in a e-commerce, browsing history, search patterns, even mouse movements performed by the user. Implicit feedback lacks the cold start problem: once the user gives approval to collect the data, no additional explicit feedback is required on the user’s part. Still, it also shows some new issues:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;No negative feedback&lt;/strong&gt;: concentrating only on gathered data leaves only with positive feedback, mispresenting the user profile.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Implicit feedback has too much noise&lt;/strong&gt;, or too much room for guessing.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Need for numerical values of implicit feedback to indicate confidence&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Appropiate measures&lt;/strong&gt; that take availability, competition and repeated feedback into account.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The model proposed by the authors follows a method similar to the one seen in matrix factorization algorithms, where the goal is to find a vector for each user and a vector for each item that will factor user preferences, where these preferences are the inner products between those vectors. They also formalize the notion of the confidence which the variables need to measure, assessing the need for wide and greatly varying confidence levels, so there is a stronger indication that a user indeed likes an item.&lt;/p&gt;

&lt;p&gt;Finally, the idea was to map users and items into a common latent factor space where they can be directly compared, and optimization had to take all possible user-item pairs into account, not only those corresponding to observed data. As the user-item matrix could easily reach billions of cells, they devised an alternating least squares optimization process, managing to make it scale linearly with the size of the data by using matrix factorizations and linear algebra.&lt;/p&gt;

&lt;h2 id=&quot;comments-and-critics&quot;&gt;Comments and critics&lt;/h2&gt;
&lt;p&gt;What was most interesting was the author’s concern of making transparent recommendations to the user, by saying that a good recommendation should be accompanied with an explanation. They managed to reduce the latent factor model into a linear model that predicts preferences as a linear function of past actions, weighted by item-item similarity.&lt;/p&gt;

&lt;p&gt;It would be interesting to know of other extensions to this model. It was first tested being implemented in a large scale TV recommender system, but how does it adapt when used in other contexts? Given that watching TV involves many behavioral scenarios that have been studied by psychology, inferring implicit feedback can be strongly supported by theory. But what happens, for example, when applied in e-commerce recommender systems, music, or even tourism? I find that even though the model is robust in terms of managing generic implicit feedback, it lacks explanations on how to apply the model to other contexts.&lt;/p&gt;
</content>
<author>
<name>thomashepner</name>
</author>
<category term="implicit" />
<category term="feedback" />
<category term="recsys" />
<summary>Paper’s contentHistoric recommender systems studies have come to distinguish two separate approaches for predicting and recommending items to a user: content-based and collaborative filtering recommendations, each with preferred applications when it comes to analyzing the dataset available or the need of the user. In this investigation, the authors introduced a new way of incorporating user data to collaborative filtering, in order to improve its accuracy, and also to assess problems such as cold start.</summary>
</entry>
<entry>
<title>Week 10 Comments: Recommender Systems: from Algorithms to User Experience</title>
<link href="http://thomashepner.github.io/week-10-comments-recommender-systems-from-algorithms-to-user-experience/" rel="alternate" type="text/html" title="Week 10 Comments: Recommender Systems: from Algorithms to User Experience" />
<published>2016-09-24T16:00:00-03:00</published>
<updated>2016-09-24T16:00:00-03:00</updated>
<id>http://thomashepner.github.io/week-10-comments-recommender-systems-from-algorithms-to-user-experience</id>
<content type="html" xml:base="http://thomashepner.github.io/week-10-comments-recommender-systems-from-algorithms-to-user-experience/">&lt;h2 id=&quot;papers-content&quot;&gt;Paper’s content&lt;/h2&gt;
&lt;p&gt;The present paper works as a review of recommender systems history, making an introduction to the first approaches to solve the recommendation problem. Besides briefly mentioning the first algorithms studied, and early applications to this new kind of technology, they introduce four categories where research has been done regarding the user experience on recommender systems topic.&lt;/p&gt;

&lt;p&gt;The first group is the &lt;strong&gt;user-recommender lifecycle&lt;/strong&gt;, and talks about how these systems can adapt to different user needs. For example, new users have different needs than experienced ones, therefore each type can provide different types of value into the system. These considerations should be taken into account when handling new users, recognizing a user’s lifecycle, and viewing the service the system provides to a community.&lt;/p&gt;

&lt;p&gt;The second group of important research involves more &lt;strong&gt;sophisticated notions of quality&lt;/strong&gt;, trying to go a little further than the average error metric, as it misses the features that are most important to user satisfaction in recommenders applications. One important source of research comes from discovering different sources of error, and accounting for each one in order to avoid making bad mistakes. Another source comes from the shift to top-N recommenders, made popular in early retail sales applications, where the most important work was done by using top-N precision and recall metrics in evaluating a top-N recommender. Finally, multidimensional ratings and tags and data quality also provide important sources of investigation.&lt;/p&gt;

&lt;p&gt;The third group regards &lt;strong&gt;hidden dangers&lt;/strong&gt;, or the social risks related to the wide deployment of recommenders. There are some subtle (and interesting) risks such as social relationships structure threats, so being careful with privacy, social consequences of recommenders, and the robustness and manipulation resistance is a must for anyone that wants to implement a system like this.&lt;/p&gt;

&lt;p&gt;As the fourth and last group of research they assess the topic of putting the user in control, by reducing the workload in the choosing process. Still, users are more satisfied when they are given control over how the recommender works on their behalf. Topics on this research group are interactive recommendation dialogues, recommender context, group recommendations, explanations and transparency.&lt;/p&gt;

&lt;h2 id=&quot;comments-and-critics&quot;&gt;Comments and critics&lt;/h2&gt;
&lt;p&gt;As educative as this research may be, the part that I found most interesting was the &lt;strong&gt;looking forward&lt;/strong&gt; section, especially the part where they mention the upcoming challenges in the field. One topic that has always interested me is how complex systems scale, in a word where information needs are very demanding and in constant growth. Still, I think this paper didn’t go into the scalability issue too deeply. For example, it would’ve been interesting to see how the integration of new attributes in recommendation affects scalability, and what kind of research has been done in that area. On the other hand, even if this challenges remain the same as years go by, they don’t mention any new ones that may have rose as a result of recommendation becoming more complex.&lt;/p&gt;

&lt;p&gt;By doing a little research myself I found two interesting investigations in this area. Even though they don’t relate to this paper’s main topic of categorizing recommender systems research, they give notion on what things are being researched today. In an investigation made by MIT academics (https://dspace.mit.edu/handle/1721.1/99785), a new matrix completion algorithm is introduced that proves effective in attacking the scalability problem. On the other hand, on another investigation (http://dl.acm.org/citation.cfm?id=2792842) they investigate machine learning methods to make &lt;em&gt;Elasticsearch&lt;/em&gt; a better recommender. Not going any further into the investigations’ content, we can see that there’s still research being done in coming up with better techniques for matrix factorizations or clustering, in order to make recommendation systems more scalable.&lt;/p&gt;
</content>
<author>
<name>thomashepner</name>
</author>
<category term="algorithms" />
<category term="ux" />
<category term="recsys" />
<summary>Paper’s contentThe present paper works as a review of recommender systems history, making an introduction to the first approaches to solve the recommendation problem. Besides briefly mentioning the first algorithms studied, and early applications to this new kind of technology, they introduce four categories where research has been done regarding the user experience on recommender systems topic.</summary>
</entry>
<entry>
<title>Week 5 Comments: Matrix Factorization Techniques for Recommender Systems</title>
<link href="http://thomashepner.github.io/week-5-comments-matrix-factorization-techniques-for-recommender-systems/" rel="alternate" type="text/html" title="Week 5 Comments: Matrix Factorization Techniques for Recommender Systems" />
<published>2016-09-11T21:00:00-03:00</published>
<updated>2016-09-11T21:00:00-03:00</updated>
<id>http://thomashepner.github.io/week-5-comments-matrix-factorization-techniques-for-recommender-systems</id>
<content type="html" xml:base="http://thomashepner.github.io/week-5-comments-matrix-factorization-techniques-for-recommender-systems/">&lt;h2 id=&quot;papers-content&quot;&gt;Paper’s content&lt;/h2&gt;
&lt;p&gt;This article shows how matrix factorization techniques can be used to improve performance in recommender systems, allowing incorporation of implicit feedback and other sources of knowledge.&lt;/p&gt;

&lt;p&gt;In this approach, items and users are represented as vectors of factors inferred from the item rating patterns, and correspondence between items and user factors results in a recommendation. This allows incorporation of additional information, when explicit feedback is not available (most common scenario). As implicit feedback denotes the &lt;strong&gt;presence or absence of an event&lt;/strong&gt;, it can be represented as a densely filled matrix.&lt;/p&gt;

&lt;p&gt;This model is closely related with SVD (&lt;em&gt;singular value decomposition&lt;/em&gt;), a well-known method for identifying latent semantic factors in information retrieval. Still, this method requires for factoring the user-item matrix, which can be prone to errors due to the high portion of missing values (sparse matrix).&lt;/p&gt;

&lt;p&gt;In order to minimize the regularized squared error on the set of known ratings, two machine learning approaches are user: &lt;strong&gt;stochastic gradient descent&lt;/strong&gt; and &lt;strong&gt;alternating least squares&lt;/strong&gt;. The first one is easy to implement with relatively good running time, while the other is highly preferable in systems that can provide parallelization, and in those that rely on implicit data.&lt;/p&gt;

&lt;p&gt;As most systems present the “cold-start” problem, it is helpful to incorporate additional sources of information, and matrix factorization delivers a way of doing so, integrating signal sources with enhanced user representation, modifying the user factor vectors.&lt;/p&gt;

&lt;h2 id=&quot;comments-and-critics&quot;&gt;Comments and critics&lt;/h2&gt;
&lt;p&gt;I found particularly interesting that, in using ALS as the learning algorithm, the process could be parallelized, as it is a feature needed in most systems these days. Doing further research, I found that a method called coordinate descent (CDD++) can be used for parallelizing matrix factorizations, and is scalable and efficient. It has lower time complexity per iteration than ALS, ando also achieves faster and more stable convergence than stochastic gradient descent, for larger scale matrices. More importantly, it can be parallelized in multi-core and distributed environments, therefore handling large-scale datasets and variables cannot fit in a single machine’s memory. More on this approach can be found in http://www.cs.utexas.edu/~inderjit/public_papers/kais-pmf.pdf.&lt;/p&gt;

&lt;p&gt;What was more interesting was that even as the number of processors scale, CDD++ still performs way better than the other learning algorithms. For example, using 256 processors it is 40 times faster than DSGD and 20 times faster than ALS.&lt;/p&gt;
</content>
<author>
<name>thomashepner</name>
</author>
<category term="content" />
<category term="recsys" />
<summary>Paper’s contentThis article shows how matrix factorization techniques can be used to improve performance in recommender systems, allowing incorporation of implicit feedback and other sources of knowledge.</summary>
</entry>
<entry>
<title>Week 5 Comments: Hybrid Recommender Systems: Survey and Experiments</title>
<link href="http://thomashepner.github.io/week-5-comments-hybrid-recommender-systems-survey-and-experiments/" rel="alternate" type="text/html" title="Week 5 Comments: Hybrid Recommender Systems: Survey and Experiments" />
<published>2016-09-11T21:00:00-03:00</published>
<updated>2016-09-11T21:00:00-03:00</updated>
<id>http://thomashepner.github.io/week-5-comments-hybrid-recommender-systems-survey-and-experiments</id>
<content type="html" xml:base="http://thomashepner.github.io/week-5-comments-hybrid-recommender-systems-survey-and-experiments/">&lt;h2 id=&quot;papers-content&quot;&gt;Paper’s content&lt;/h2&gt;

&lt;p&gt;Hybrid recommender systems combine two or more recommendation techniques to gain better performance with fewer of the drawbacks of any individual one. It is common to see collaborative filtering combined with other approaches when seeing this kind of systems. They can be catalogued in &lt;strong&gt;weighted&lt;/strong&gt;, where the score of an item results from al the available recommendation techniques present in the system; &lt;strong&gt;switching&lt;/strong&gt;, where a certain criterion is used to switch between approaches; &lt;strong&gt;mixed&lt;/strong&gt;, used when it is practical to make a large number of recommendations simultaneously, showing recommendations from more than one technique; &lt;strong&gt;feature combination&lt;/strong&gt;, treating collaborative information as an additional feature, and then using content-based algorithms over the dataset; &lt;strong&gt;cascade&lt;/strong&gt;, involving a staged process where additional recommendation techniques are used to refine the result; &lt;strong&gt;feature augmentation&lt;/strong&gt;, where one technique is employed to produce a classification, information then used for another recommendation technique; and &lt;strong&gt;meta-level&lt;/strong&gt;, using the model generated by the first technique as input to the next one.&lt;/p&gt;

&lt;p&gt;The paper introduces a recommendation engine where they combine knowledge-based/collaborative in a cascade manner, and conduct a series of experiments with it. In analyzing the results of the experiment, the researchers prove that there is a synergy between knowledge-based and collaborative filtering. More importantly, the ramp-up problem characteristic to CF is reduced, as this technique comes second in the cascade, therefore showing benefits to the casual user, and still improving its recommendations over time.&lt;/p&gt;

&lt;h2 id=&quot;comments-and-critics&quot;&gt;Comments and critics&lt;/h2&gt;

&lt;p&gt;This investigation makes use of a similarity metric between ratings based on their characteristics, regarded as a &lt;em&gt;heuristic similarity&lt;/em&gt; approach, that does not establish a single numeric scale to which all actions are converted, but looks at the &lt;strong&gt;similarity of users on a rating by rating basis&lt;/strong&gt;. I found this very interesting, because all rating scales that I had seen prior to this established a standard numeric value where each user appreciation could be transformed into. Also, in doing so they establish assumptions that are strong, but paired with common sense.&lt;/p&gt;

&lt;p&gt;In order to use this kind of heuristic, it is important to define the semantics of the vocabulary correctly, as it is very sensitive to each value’s sign. Still, it is important to see the trade-off between the benefits using this metric, and the drawbacks of defining opposites correctly. I think it may be very hard to think of every combination and all possible comparisons between features, and it is most definitively not scalable. In my opinion, and as we can see in the experiments, a suitable choice for using this kind of heuristic is in restaurants, movies, and small e-commerces, where data is well structured and features can be easily categorized and compared.&lt;/p&gt;
</content>
<author>
<name>thomashepner</name>
</author>
<category term="content" />
<category term="recsys" />
<summary>Paper’s content</summary>
</entry>
<entry>
<title>Week 4 Comments: Content-based Recommendation Systems</title>
<link href="http://thomashepner.github.io/week-4-comments-content-based-recommendation-systems/" rel="alternate" type="text/html" title="Week 4 Comments: Content-based Recommendation Systems" />
<published>2016-09-04T16:00:00-03:00</published>
<updated>2016-09-04T16:00:00-03:00</updated>
<id>http://thomashepner.github.io/week-4-comments-content-based-recommendation-systems</id>
<content type="html" xml:base="http://thomashepner.github.io/week-4-comments-content-based-recommendation-systems/">&lt;h2 id=&quot;papers-content&quot;&gt;Paper’s content&lt;/h2&gt;
&lt;p&gt;Content-based recommender systems follow a different approach from the ones we’ve studied so far. They analyze an item’s description to identify others that would be interesting to a certain user. As a system of this kind would need to analyze thousands of items in short periods of time, in order to recommend accurately to a user, every part of the process should be as efficient as possible. Therefore, it is of interest to represent each item in a way that it makes possible to process easily.&lt;/p&gt;

&lt;p&gt;A common approach to represent unstructured text in an item’s description is to convert the free text to a structured representation. For example, each word can be an attribute of the document, with an integer value for its frequency or maybe a boolean value for appearance. There’s a technique called &lt;strong&gt;stemming&lt;/strong&gt;, where rather than using words, the &lt;strong&gt;root forms of words are stored&lt;/strong&gt;, creating a term that reflects the common meaning of similar words. A value, called &lt;strong&gt;&lt;em&gt;tf-idf&lt;/em&gt; weight&lt;/strong&gt;, is used to identify the frequency of each term in a document, relevant to a document collection. Using this value a recommender system can tell which terms are more central to the topic of the document.&lt;/p&gt;

&lt;p&gt;To save a user’s preferences, the system saves user profiles with different types of information, storing ways to retrieve items that are of his/her interest, like a history of the user’s interaction with the system, customization settings, and even &lt;strong&gt;rules to recommend other product based on history&lt;/strong&gt;. In doing so, there are several tasks related to classification learning, using implicit and explicit feedback to construct a user’s model. It is important to note that &lt;strong&gt;implicit feedback can collect a large amount of data, but with some uncertainty&lt;/strong&gt;. Finally, the learning algorithm delivers a function that provides an estimate of the probability that a user will like an unseen item.&lt;/p&gt;

&lt;p&gt;Several methods and approaches for performing content-based recommendation are explained throughout the paper. They explain how &lt;strong&gt;decision trees and rule induction&lt;/strong&gt; are used to represent an item’s description; how nearest neighbor methods compares similarity by using different functions depending on the type of data; and how relevance feedback can help improve recommendations based on previous queries performed by the user.&lt;/p&gt;

&lt;p&gt;Finally, the paper introduces probabilistic methods and naïve Bayes methods for performing text classification in a probabilistic approach. Even though naïve Bayes’ assumption of class-conditional attribute independence is violated in this context, the algorithm still performs very well, and it has been proven that this condition is not necessary for optimality.&lt;/p&gt;

&lt;h2 id=&quot;comments-and-critics&quot;&gt;Comments and critics&lt;/h2&gt;
&lt;p&gt;This investigation’s final conclusion is that &lt;strong&gt;no content-based recommender system can give good recommendations if the content does not contain enough information to distinguish items the user likes from those the user doesn’t like&lt;/strong&gt;. Good examples that contain this kind of information are movies, restaurants and television. Still, better results are found by complementing with additional information. For example, including the opinion of other users by adding data associated to the representation of the examples.&lt;/p&gt;

&lt;p&gt;What I found most interesting though was that a content-based approach can be used alongside a collaborative-filtering system to produce better recommendations. For example, a e-book store such as Amazon can use collaborative-filtering to recommend items that a user would like, but then filter them by categories using content-based algorithms. This ways it can leave out categories such as adult’s literature when recommending to a younger user.&lt;/p&gt;

&lt;p&gt;I was interested in finding out who uses the content-based approach in their systems nowadays, and found out that music streaming service &lt;strong&gt;Pandora&lt;/strong&gt; uses machine learning methods to recommend by content, as it is &lt;strong&gt;not interested in relating the interest of one user to another&lt;/strong&gt;. &lt;span class=&quot;evidence&quot;&gt;Pandora does not rely on user’s opinions: only the musical classification of a song determines whether a station will play it. The service looks for music whose content structure is similar to the taste of the user, and chooses the songs closest to that definition, with their own filtering techniques.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Pandora’s system key method is to use an efficient proximity measure to determine neighborhoods for a radio station. As the number of songs in the platform remains fairly static, the algorithm performs in polynomial time. More so, the most important factor is the time it takes to choose the next song to play from the current station, and it can be done in &lt;em&gt;O(mn)&lt;/em&gt; time, where &lt;em&gt;m&lt;/em&gt; is the number of artists/songs in the station, and &lt;em&gt;n&lt;/em&gt; is the number of attributes that characterizes a song’s structure.&lt;/p&gt;

&lt;p&gt;Applications like Pandora are a good example of how this type of recommendation is still relevant and worth studying.&lt;/p&gt;
</content>
<author>
<name>thomashepner</name>
</author>
<category term="content" />
<category term="recsys" />
<summary>Paper’s contentContent-based recommender systems follow a different approach from the ones we’ve studied so far. They analyze an item’s description to identify others that would be interesting to a certain user. As a system of this kind would need to analyze thousands of items in short periods of time, in order to recommend accurately to a user, every part of the process should be as efficient as possible. Therefore, it is of interest to represent each item in a way that it makes possible to process easily.</summary>
</entry>
<entry>
<title>Week 4 Comments: Collaborative Filtering for Social Tagging Systems: an Experiment with CiteULike</title>
<link href="http://thomashepner.github.io/week-4-comments-collaborative-filtering-for-social-tagging-systems-an-experiment-with-citeulike/" rel="alternate" type="text/html" title="Week 4 Comments: Collaborative Filtering for Social Tagging Systems: an Experiment with CiteULike" />
<published>2016-09-04T16:00:00-03:00</published>
<updated>2016-09-04T16:00:00-03:00</updated>
<id>http://thomashepner.github.io/week-4-comments-collaborative-filtering-for-social-tagging-systems-an-experiment-with-citeulike</id>
<content type="html" xml:base="http://thomashepner.github.io/week-4-comments-collaborative-filtering-for-social-tagging-systems-an-experiment-with-citeulike/">&lt;h2 id=&quot;papers-content&quot;&gt;Paper’s content&lt;/h2&gt;
&lt;p&gt;As user-contributed content is becoming more diverse in nature and quality, and traditional 5-10 point ratings are mostly unavailable, there are some new challenges for collaborative tagging systems. The loss of quality control and fine-grained ratings in these systems can be compensated by tags and explicit connections between users. Still, the paper states that there was no standard in how to take these features into account. There is some evidence, though, that social links improve personalization quality.&lt;/p&gt;

&lt;p&gt;The experiment conducted during this investigation consisted in crawling data from CiteULike: first, crawl a user’s posted articles. Then, calculate neighborhoods of users who posted the same articles and who share the same tags. Finally, use Krovetz’ algorithm to modify tags.&lt;/p&gt;

&lt;p&gt;The algorithm tested consisted of two steps: find the neighborhood of the center user, and rank the articles to be recommended. They used three different approaches: &lt;strong&gt;Classic Collaborative Filtering&lt;/strong&gt;, &lt;strong&gt;Neighbor-weighted CF&lt;/strong&gt;, and &lt;strong&gt;BM25-based Similarity&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;While testing this different methods, they discovered that, as predicted, CCF performed worse than the other algorithms, that where very close in terms of performance. The results also led to find out that the ranking order of the recommendations was very close to the optimal one.&lt;/p&gt;

&lt;p&gt;An important conclusion was that &lt;strong&gt;including the number of raters in the ranking formula is an important factor to consider in the success of these recommendations&lt;/strong&gt;, as it takes into account the weight a good rater has, compared too poor raters of a certain article. In doing so, this practice reduces the uncertainty produced by items with too few ratings.&lt;/p&gt;

&lt;p&gt;As Pearson’s correlation performs badly in this context, tag-based approaches can be a suitable alternative to obtain user neighborhood in social tagging systems.&lt;/p&gt;

&lt;h2 id=&quot;comments-and-critics&quot;&gt;Comments and critics&lt;/h2&gt;
&lt;p&gt;The paper introduces the BM25-based formula for calculating similarity, a non-binary probabilistic method used in information retrieval that returns the relevance that the documents of one collection has given a query. It compares the tags of the center user with a query, and the set of tags of each neighbor as a document. This method was first used in the Okapi BM25 information retrieval system in the 1980s.&lt;/p&gt;

&lt;p&gt;There are several extensions of the algorithm, like BM25F and BM25+. The first one considers the document to be composed from several fields, like headers and main text; the second addresses unfairly scored documents that have a similar relevance to shorter documents that do not contain the search query at all.&lt;/p&gt;

&lt;p&gt;I think the use of information retrieval techniques is a good choice in this kind of systems, especially because tags are a structured form of information, therefore making it easier to retrieve information and rank recommendations.&lt;/p&gt;
</content>
<author>
<name>thomashepner</name>
</author>
<category term="content" />
<category term="recsys" />
<summary>Paper’s contentAs user-contributed content is becoming more diverse in nature and quality, and traditional 5-10 point ratings are mostly unavailable, there are some new challenges for collaborative tagging systems. The loss of quality control and fine-grained ratings in these systems can be compensated by tags and explicit connections between users. Still, the paper states that there was no standard in how to take these features into account. There is some evidence, though, that social links improve personalization quality.</summary>
</entry>
</feed>
