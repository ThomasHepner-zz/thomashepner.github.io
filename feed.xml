<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
<generator uri="http://jekyllrb.com" version="3.0.3">Jekyll</generator>
<link href="http://thomashepner.github.io/feed.xml" rel="self" type="application/atom+xml" />
<link href="http://thomashepner.github.io/" rel="alternate" type="text/html" />
<updated>2016-09-30T11:29:15-03:00</updated>
<id>http://thomashepner.github.io/</id>
<title>Thomas Hepner</title>
<subtitle>Thoughts on RecSys readings and papers.</subtitle>
<entry>
<title>Week 10 Comments: Context-Aware Recommender Systems</title>
<link href="http://thomashepner.github.io/week-10-comments-context-aware-recommender-systems/" rel="alternate" type="text/html" title="Week 10 Comments: Context-Aware Recommender Systems" />
<published>2016-09-30T10:00:00-03:00</published>
<updated>2016-09-30T10:00:00-03:00</updated>
<id>http://thomashepner.github.io/week-10-comments-context-aware-recommender-systems</id>
<content type="html" xml:base="http://thomashepner.github.io/week-10-comments-context-aware-recommender-systems/">&lt;h2 id=&quot;papers-content&quot;&gt;Paper’s content&lt;/h2&gt;

&lt;p&gt;This investigation proposes a recommender system that also uses the context in which the rating happened to include this potentially useful information into the knowledge stream. The goal is to label every user action with an appropriate context, and then build a system that makes good use of it. Still, it is difficult to define context because of its multifaceted nature, and there are many questions to be answered in order to apply this kind of information into a recommendation. There are two views to context: &lt;strong&gt;representational&lt;/strong&gt; and &lt;strong&gt;interactional&lt;/strong&gt; contexts, where the first is independent from the underlying activity being performed, whereas the second assumes a cyclical relationship between context and activity, each one influencing the other over and over again.&lt;/p&gt;

&lt;p&gt;The approach followed by the authors makes use of &lt;strong&gt;contextual factors&lt;/strong&gt;, such as time, location, and purchasing purpose. These factors can be catalogued by the knowledge we have of them, and by how they change (static, dynamic). This two categories define a category matrix, and the authors focus only on static, fully observable context. In that manner, they define a model in which a new dimension is used to calculate ratings:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;R: Users&lt;/em&gt; x &lt;em&gt;Items&lt;/em&gt; x &lt;em&gt;Contexts&lt;/em&gt; -&amp;gt; &lt;em&gt;Ratings&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This model raises the issue of scalability right away: it needs to include a way of computing similarities efficiently, given that three dimensions would normally lead to &lt;em&gt;O(n^3)&lt;/em&gt; complexities.  In order to filter the context, three approaches are proposed: contextual prefiltering, contextual postfiltering, and contextual modeling.&lt;/p&gt;

&lt;p&gt;Contextual prefiltering uses contextual information to select the most relevant User x Item tuples for generating recommendations, where the context serves as a query for selecting the data. However, the exact context sometimes can be too narrow, and the exact context may not have enough data for accurate predictions. Later investigations assess this issue by splitting user profiles into several sub-profiles that represent the user in different problems. This, however, raises the overlapping user profiles problem.&lt;/p&gt;

&lt;p&gt;Contextual postfiltering ignores context information until a recommendation is made, and adjusts the obtained recommendation list to the context afterwards, using either a model-based or a heuristics-based approach. A major advantage of this is that it allows using any traditional recommendation technique to provide the top-N item list for a user.&lt;/p&gt;

&lt;p&gt;Contextual modeling uses context directly in the recommendation function, allowing truly multidimensional recommendation functions that may rely on decision trees, regresions, or even heuristics that incorporate contextual information. In this type of modeling it has been used a tensor based approach, in response to the popular matrix factorization method, but allowing for &lt;em&gt;n&lt;/em&gt;-dimensions in the calculations. Further research shows that context-aware SVM outperforms noncontextual SVM in terms of predictive accuracy.&lt;/p&gt;

&lt;h2 id=&quot;comments-and-critics&quot;&gt;Comments and critics&lt;/h2&gt;
&lt;p&gt;I found the paper very complete, although I missed some comparisons between noncontextual methods and CARS, specially in the POI example they provided. Still, it is interesting how they sort of “predict” the applications in which this topic would’ve involved in the future.
A couple of students from the RecSys course at PUC Chile did a comparison between implicit feedback and context-aware recommendations, where they compared Hu and Koren’s implicit feedback algorithm with tensor factorization and factorization machines in order to see tradeoffs between these approaches, using the MovieCity dataset. Even though factorization machines rating prediction outperformed the other approaches, it was interesting to find out that the implicit feedback algorithm was better in making a ranking of recommendations, therefore concluding that context not necessarily improves a top-N task.
Applied to this paper, it would seem that a combination between contextual modeling and implicit feedback could yield an optimal combination towards making recommendations to a user: using the context to make predictions, and then including implicit feedback to make a top-N recommendation. It would be interesting to see this approach in further research, building a hybrid system that uses both context and implicit feedback in the recommendation engine.&lt;/p&gt;
</content>
<author>
<name>thomashepner</name>
</author>
<category term="context" />
<category term="aware" />
<category term="recsys" />
<summary>Paper’s content</summary>
</entry>
<entry>
<title>Week 9 Comments: Recommender Systems: from Algorithms to User Experience</title>
<link href="http://thomashepner.github.io/week-9-comments-recommender-systems-from-algorithms-to-user-experience/" rel="alternate" type="text/html" title="Week 9 Comments: Recommender Systems: from Algorithms to User Experience" />
<published>2016-09-24T16:00:00-03:00</published>
<updated>2016-09-24T16:00:00-03:00</updated>
<id>http://thomashepner.github.io/week-9-comments-recommender-systems-from-algorithms-to-user-experience</id>
<content type="html" xml:base="http://thomashepner.github.io/week-9-comments-recommender-systems-from-algorithms-to-user-experience/">&lt;h2 id=&quot;papers-content&quot;&gt;Paper’s content&lt;/h2&gt;
&lt;p&gt;The present paper works as a review of recommender systems history, making an introduction to the first approaches to solve the recommendation problem. Besides briefly mentioning the first algorithms studied, and early applications to this new kind of technology, they introduce four categories where research has been done regarding the user experience on recommender systems topic.&lt;/p&gt;

&lt;p&gt;The first group is the &lt;strong&gt;user-recommender lifecycle&lt;/strong&gt;, and talks about how these systems can adapt to different user needs. For example, new users have different needs than experienced ones, therefore each type can provide different types of value into the system. These considerations should be taken into account when handling new users, recognizing a user’s lifecycle, and viewing the service the system provides to a community.&lt;/p&gt;

&lt;p&gt;The second group of important research involves more &lt;strong&gt;sophisticated notions of quality&lt;/strong&gt;, trying to go a little further than the average error metric, as it misses the features that are most important to user satisfaction in recommenders applications. One important source of research comes from discovering different sources of error, and accounting for each one in order to avoid making bad mistakes. Another source comes from the shift to top-N recommenders, made popular in early retail sales applications, where the most important work was done by using top-N precision and recall metrics in evaluating a top-N recommender. Finally, multidimensional ratings and tags and data quality also provide important sources of investigation.&lt;/p&gt;

&lt;p&gt;The third group regards &lt;strong&gt;hidden dangers&lt;/strong&gt;, or the social risks related to the wide deployment of recommenders. There are some subtle (and interesting) risks such as social relationships structure threats, so being careful with privacy, social consequences of recommenders, and the robustness and manipulation resistance is a must for anyone that wants to implement a system like this.&lt;/p&gt;

&lt;p&gt;As the fourth and last group of research they assess the topic of putting the user in control, by reducing the workload in the choosing process. Still, users are more satisfied when they are given control over how the recommender works on their behalf. Topics on this research group are interactive recommendation dialogues, recommender context, group recommendations, explanations and transparency.&lt;/p&gt;

&lt;h2 id=&quot;comments-and-critics&quot;&gt;Comments and critics&lt;/h2&gt;
&lt;p&gt;As educative as this research may be, the part that I found most interesting was the &lt;strong&gt;looking forward&lt;/strong&gt; section, especially the part where they mention the upcoming challenges in the field. One topic that has always interested me is how complex systems scale, in a word where information needs are very demanding and in constant growth. Still, I think this paper didn’t go into the scalability issue too deeply. For example, it would’ve been interesting to see how the integration of new attributes in recommendation affects scalability, and what kind of research has been done in that area. On the other hand, even if this challenges remain the same as years go by, they don’t mention any new ones that may have rose as a result of recommendation becoming more complex.&lt;/p&gt;

&lt;p&gt;By doing a little research myself I found two interesting investigations in this area. Even though they don’t relate to this paper’s main topic of categorizing recommender systems research, they give notion on what things are being researched today. In an investigation made by MIT academics (https://dspace.mit.edu/handle/1721.1/99785), a new matrix completion algorithm is introduced that proves effective in attacking the scalability problem. On the other hand, on another investigation (http://dl.acm.org/citation.cfm?id=2792842) they investigate machine learning methods to make &lt;em&gt;Elasticsearch&lt;/em&gt; a better recommender. Not going any further into the investigations’ content, we can see that there’s still research being done in coming up with better techniques for matrix factorizations or clustering, in order to make recommendation systems more scalable.&lt;/p&gt;
</content>
<author>
<name>thomashepner</name>
</author>
<category term="algorithms" />
<category term="ux" />
<category term="recsys" />
<summary>Paper’s contentThe present paper works as a review of recommender systems history, making an introduction to the first approaches to solve the recommendation problem. Besides briefly mentioning the first algorithms studied, and early applications to this new kind of technology, they introduce four categories where research has been done regarding the user experience on recommender systems topic.</summary>
</entry>
<entry>
<title>Week 6 Comments: Collaborative Filtering for Implicit Feedback Datasets</title>
<link href="http://thomashepner.github.io/week-6-comments-collaborative-filtering-for-implicit-feedback-datasets/" rel="alternate" type="text/html" title="Week 6 Comments: Collaborative Filtering for Implicit Feedback Datasets" />
<published>2016-09-24T16:00:00-03:00</published>
<updated>2016-09-24T16:00:00-03:00</updated>
<id>http://thomashepner.github.io/week-6-comments-collaborative-filtering-for-implicit-feedback-datasets</id>
<content type="html" xml:base="http://thomashepner.github.io/week-6-comments-collaborative-filtering-for-implicit-feedback-datasets/">&lt;h2 id=&quot;papers-content&quot;&gt;Paper’s content&lt;/h2&gt;
&lt;p&gt;Historic recommender systems studies have come to distinguish two separate approaches for predicting and recommending items to a user: content-based and collaborative filtering recommendations, each with preferred applications when it comes to analyzing the dataset available or the need of the user. In this investigation, the authors introduced a new way of incorporating user data to collaborative filtering, in order to improve its accuracy, and also to assess problems such as cold start.&lt;/p&gt;

&lt;p&gt;As explicit feedback is not always available in a dataset, they proposed a way of using a much more abundant source of data: implicit feedback. This could manifest in the shape of purchase history in a e-commerce, browsing history, search patterns, even mouse movements performed by the user. Implicit feedback lacks the cold start problem: once the user gives approval to collect the data, no additional explicit feedback is required on the user’s part. Still, it also shows some new issues:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;No negative feedback&lt;/strong&gt;: concentrating only on gathered data leaves only with positive feedback, mispresenting the user profile.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Implicit feedback has too much noise&lt;/strong&gt;, or too much room for guessing.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Need for numerical values of implicit feedback to indicate confidence&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Appropiate measures&lt;/strong&gt; that take availability, competition and repeated feedback into account.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The model proposed by the authors follows a method similar to the one seen in matrix factorization algorithms, where the goal is to find a vector for each user and a vector for each item that will factor user preferences, where these preferences are the inner products between those vectors. They also formalize the notion of the confidence which the variables need to measure, assessing the need for wide and greatly varying confidence levels, so there is a stronger indication that a user indeed likes an item.&lt;/p&gt;

&lt;p&gt;Finally, the idea was to map users and items into a common latent factor space where they can be directly compared, and optimization had to take all possible user-item pairs into account, not only those corresponding to observed data. As the user-item matrix could easily reach billions of cells, they devised an alternating least squares optimization process, managing to make it scale linearly with the size of the data by using matrix factorizations and linear algebra.&lt;/p&gt;

&lt;h2 id=&quot;comments-and-critics&quot;&gt;Comments and critics&lt;/h2&gt;
&lt;p&gt;What was most interesting was the author’s concern of making transparent recommendations to the user, by saying that a good recommendation should be accompanied with an explanation. They managed to reduce the latent factor model into a linear model that predicts preferences as a linear function of past actions, weighted by item-item similarity.&lt;/p&gt;

&lt;p&gt;It would be interesting to know of other extensions to this model. It was first tested being implemented in a large scale TV recommender system, but how does it adapt when used in other contexts? Given that watching TV involves many behavioral scenarios that have been studied by psychology, inferring implicit feedback can be strongly supported by theory. But what happens, for example, when applied in e-commerce recommender systems, music, or even tourism? I find that even though the model is robust in terms of managing generic implicit feedback, it lacks explanations on how to apply the model to other contexts.&lt;/p&gt;
</content>
<author>
<name>thomashepner</name>
</author>
<category term="implicit" />
<category term="feedback" />
<category term="recsys" />
<summary>Paper’s contentHistoric recommender systems studies have come to distinguish two separate approaches for predicting and recommending items to a user: content-based and collaborative filtering recommendations, each with preferred applications when it comes to analyzing the dataset available or the need of the user. In this investigation, the authors introduced a new way of incorporating user data to collaborative filtering, in order to improve its accuracy, and also to assess problems such as cold start.</summary>
</entry>
<entry>
<title>Week 5 Comments: Matrix Factorization Techniques for Recommender Systems</title>
<link href="http://thomashepner.github.io/week-5-comments-matrix-factorization-techniques-for-recommender-systems/" rel="alternate" type="text/html" title="Week 5 Comments: Matrix Factorization Techniques for Recommender Systems" />
<published>2016-09-11T21:00:00-03:00</published>
<updated>2016-09-11T21:00:00-03:00</updated>
<id>http://thomashepner.github.io/week-5-comments-matrix-factorization-techniques-for-recommender-systems</id>
<content type="html" xml:base="http://thomashepner.github.io/week-5-comments-matrix-factorization-techniques-for-recommender-systems/">&lt;h2 id=&quot;papers-content&quot;&gt;Paper’s content&lt;/h2&gt;
&lt;p&gt;This article shows how matrix factorization techniques can be used to improve performance in recommender systems, allowing incorporation of implicit feedback and other sources of knowledge.&lt;/p&gt;

&lt;p&gt;In this approach, items and users are represented as vectors of factors inferred from the item rating patterns, and correspondence between items and user factors results in a recommendation. This allows incorporation of additional information, when explicit feedback is not available (most common scenario). As implicit feedback denotes the &lt;strong&gt;presence or absence of an event&lt;/strong&gt;, it can be represented as a densely filled matrix.&lt;/p&gt;

&lt;p&gt;This model is closely related with SVD (&lt;em&gt;singular value decomposition&lt;/em&gt;), a well-known method for identifying latent semantic factors in information retrieval. Still, this method requires for factoring the user-item matrix, which can be prone to errors due to the high portion of missing values (sparse matrix).&lt;/p&gt;

&lt;p&gt;In order to minimize the regularized squared error on the set of known ratings, two machine learning approaches are user: &lt;strong&gt;stochastic gradient descent&lt;/strong&gt; and &lt;strong&gt;alternating least squares&lt;/strong&gt;. The first one is easy to implement with relatively good running time, while the other is highly preferable in systems that can provide parallelization, and in those that rely on implicit data.&lt;/p&gt;

&lt;p&gt;As most systems present the “cold-start” problem, it is helpful to incorporate additional sources of information, and matrix factorization delivers a way of doing so, integrating signal sources with enhanced user representation, modifying the user factor vectors.&lt;/p&gt;

&lt;h2 id=&quot;comments-and-critics&quot;&gt;Comments and critics&lt;/h2&gt;
&lt;p&gt;I found particularly interesting that, in using ALS as the learning algorithm, the process could be parallelized, as it is a feature needed in most systems these days. Doing further research, I found that a method called coordinate descent (CDD++) can be used for parallelizing matrix factorizations, and is scalable and efficient. It has lower time complexity per iteration than ALS, ando also achieves faster and more stable convergence than stochastic gradient descent, for larger scale matrices. More importantly, it can be parallelized in multi-core and distributed environments, therefore handling large-scale datasets and variables cannot fit in a single machine’s memory. More on this approach can be found in http://www.cs.utexas.edu/~inderjit/public_papers/kais-pmf.pdf.&lt;/p&gt;

&lt;p&gt;What was more interesting was that even as the number of processors scale, CDD++ still performs way better than the other learning algorithms. For example, using 256 processors it is 40 times faster than DSGD and 20 times faster than ALS.&lt;/p&gt;
</content>
<author>
<name>thomashepner</name>
</author>
<category term="content" />
<category term="recsys" />
<summary>Paper’s contentThis article shows how matrix factorization techniques can be used to improve performance in recommender systems, allowing incorporation of implicit feedback and other sources of knowledge.</summary>
</entry>
<entry>
<title>Week 5 Comments: Hybrid Recommender Systems: Survey and Experiments</title>
<link href="http://thomashepner.github.io/week-5-comments-hybrid-recommender-systems-survey-and-experiments/" rel="alternate" type="text/html" title="Week 5 Comments: Hybrid Recommender Systems: Survey and Experiments" />
<published>2016-09-11T21:00:00-03:00</published>
<updated>2016-09-11T21:00:00-03:00</updated>
<id>http://thomashepner.github.io/week-5-comments-hybrid-recommender-systems-survey-and-experiments</id>
<content type="html" xml:base="http://thomashepner.github.io/week-5-comments-hybrid-recommender-systems-survey-and-experiments/">&lt;h2 id=&quot;papers-content&quot;&gt;Paper’s content&lt;/h2&gt;

&lt;p&gt;Hybrid recommender systems combine two or more recommendation techniques to gain better performance with fewer of the drawbacks of any individual one. It is common to see collaborative filtering combined with other approaches when seeing this kind of systems. They can be catalogued in &lt;strong&gt;weighted&lt;/strong&gt;, where the score of an item results from al the available recommendation techniques present in the system; &lt;strong&gt;switching&lt;/strong&gt;, where a certain criterion is used to switch between approaches; &lt;strong&gt;mixed&lt;/strong&gt;, used when it is practical to make a large number of recommendations simultaneously, showing recommendations from more than one technique; &lt;strong&gt;feature combination&lt;/strong&gt;, treating collaborative information as an additional feature, and then using content-based algorithms over the dataset; &lt;strong&gt;cascade&lt;/strong&gt;, involving a staged process where additional recommendation techniques are used to refine the result; &lt;strong&gt;feature augmentation&lt;/strong&gt;, where one technique is employed to produce a classification, information then used for another recommendation technique; and &lt;strong&gt;meta-level&lt;/strong&gt;, using the model generated by the first technique as input to the next one.&lt;/p&gt;

&lt;p&gt;The paper introduces a recommendation engine where they combine knowledge-based/collaborative in a cascade manner, and conduct a series of experiments with it. In analyzing the results of the experiment, the researchers prove that there is a synergy between knowledge-based and collaborative filtering. More importantly, the ramp-up problem characteristic to CF is reduced, as this technique comes second in the cascade, therefore showing benefits to the casual user, and still improving its recommendations over time.&lt;/p&gt;

&lt;h2 id=&quot;comments-and-critics&quot;&gt;Comments and critics&lt;/h2&gt;

&lt;p&gt;This investigation makes use of a similarity metric between ratings based on their characteristics, regarded as a &lt;em&gt;heuristic similarity&lt;/em&gt; approach, that does not establish a single numeric scale to which all actions are converted, but looks at the &lt;strong&gt;similarity of users on a rating by rating basis&lt;/strong&gt;. I found this very interesting, because all rating scales that I had seen prior to this established a standard numeric value where each user appreciation could be transformed into. Also, in doing so they establish assumptions that are strong, but paired with common sense.&lt;/p&gt;

&lt;p&gt;In order to use this kind of heuristic, it is important to define the semantics of the vocabulary correctly, as it is very sensitive to each value’s sign. Still, it is important to see the trade-off between the benefits using this metric, and the drawbacks of defining opposites correctly. I think it may be very hard to think of every combination and all possible comparisons between features, and it is most definitively not scalable. In my opinion, and as we can see in the experiments, a suitable choice for using this kind of heuristic is in restaurants, movies, and small e-commerces, where data is well structured and features can be easily categorized and compared.&lt;/p&gt;
</content>
<author>
<name>thomashepner</name>
</author>
<category term="content" />
<category term="recsys" />
<summary>Paper’s content</summary>
</entry>
<entry>
<title>Week 4 Comments: Content-based Recommendation Systems</title>
<link href="http://thomashepner.github.io/week-4-comments-content-based-recommendation-systems/" rel="alternate" type="text/html" title="Week 4 Comments: Content-based Recommendation Systems" />
<published>2016-09-04T16:00:00-03:00</published>
<updated>2016-09-04T16:00:00-03:00</updated>
<id>http://thomashepner.github.io/week-4-comments-content-based-recommendation-systems</id>
<content type="html" xml:base="http://thomashepner.github.io/week-4-comments-content-based-recommendation-systems/">&lt;h2 id=&quot;papers-content&quot;&gt;Paper’s content&lt;/h2&gt;
&lt;p&gt;Content-based recommender systems follow a different approach from the ones we’ve studied so far. They analyze an item’s description to identify others that would be interesting to a certain user. As a system of this kind would need to analyze thousands of items in short periods of time, in order to recommend accurately to a user, every part of the process should be as efficient as possible. Therefore, it is of interest to represent each item in a way that it makes possible to process easily.&lt;/p&gt;

&lt;p&gt;A common approach to represent unstructured text in an item’s description is to convert the free text to a structured representation. For example, each word can be an attribute of the document, with an integer value for its frequency or maybe a boolean value for appearance. There’s a technique called &lt;strong&gt;stemming&lt;/strong&gt;, where rather than using words, the &lt;strong&gt;root forms of words are stored&lt;/strong&gt;, creating a term that reflects the common meaning of similar words. A value, called &lt;strong&gt;&lt;em&gt;tf-idf&lt;/em&gt; weight&lt;/strong&gt;, is used to identify the frequency of each term in a document, relevant to a document collection. Using this value a recommender system can tell which terms are more central to the topic of the document.&lt;/p&gt;

&lt;p&gt;To save a user’s preferences, the system saves user profiles with different types of information, storing ways to retrieve items that are of his/her interest, like a history of the user’s interaction with the system, customization settings, and even &lt;strong&gt;rules to recommend other product based on history&lt;/strong&gt;. In doing so, there are several tasks related to classification learning, using implicit and explicit feedback to construct a user’s model. It is important to note that &lt;strong&gt;implicit feedback can collect a large amount of data, but with some uncertainty&lt;/strong&gt;. Finally, the learning algorithm delivers a function that provides an estimate of the probability that a user will like an unseen item.&lt;/p&gt;

&lt;p&gt;Several methods and approaches for performing content-based recommendation are explained throughout the paper. They explain how &lt;strong&gt;decision trees and rule induction&lt;/strong&gt; are used to represent an item’s description; how nearest neighbor methods compares similarity by using different functions depending on the type of data; and how relevance feedback can help improve recommendations based on previous queries performed by the user.&lt;/p&gt;

&lt;p&gt;Finally, the paper introduces probabilistic methods and naïve Bayes methods for performing text classification in a probabilistic approach. Even though naïve Bayes’ assumption of class-conditional attribute independence is violated in this context, the algorithm still performs very well, and it has been proven that this condition is not necessary for optimality.&lt;/p&gt;

&lt;h2 id=&quot;comments-and-critics&quot;&gt;Comments and critics&lt;/h2&gt;
&lt;p&gt;This investigation’s final conclusion is that &lt;strong&gt;no content-based recommender system can give good recommendations if the content does not contain enough information to distinguish items the user likes from those the user doesn’t like&lt;/strong&gt;. Good examples that contain this kind of information are movies, restaurants and television. Still, better results are found by complementing with additional information. For example, including the opinion of other users by adding data associated to the representation of the examples.&lt;/p&gt;

&lt;p&gt;What I found most interesting though was that a content-based approach can be used alongside a collaborative-filtering system to produce better recommendations. For example, a e-book store such as Amazon can use collaborative-filtering to recommend items that a user would like, but then filter them by categories using content-based algorithms. This ways it can leave out categories such as adult’s literature when recommending to a younger user.&lt;/p&gt;

&lt;p&gt;I was interested in finding out who uses the content-based approach in their systems nowadays, and found out that music streaming service &lt;strong&gt;Pandora&lt;/strong&gt; uses machine learning methods to recommend by content, as it is &lt;strong&gt;not interested in relating the interest of one user to another&lt;/strong&gt;. &lt;span class=&quot;evidence&quot;&gt;Pandora does not rely on user’s opinions: only the musical classification of a song determines whether a station will play it. The service looks for music whose content structure is similar to the taste of the user, and chooses the songs closest to that definition, with their own filtering techniques.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Pandora’s system key method is to use an efficient proximity measure to determine neighborhoods for a radio station. As the number of songs in the platform remains fairly static, the algorithm performs in polynomial time. More so, the most important factor is the time it takes to choose the next song to play from the current station, and it can be done in &lt;em&gt;O(mn)&lt;/em&gt; time, where &lt;em&gt;m&lt;/em&gt; is the number of artists/songs in the station, and &lt;em&gt;n&lt;/em&gt; is the number of attributes that characterizes a song’s structure.&lt;/p&gt;

&lt;p&gt;Applications like Pandora are a good example of how this type of recommendation is still relevant and worth studying.&lt;/p&gt;
</content>
<author>
<name>thomashepner</name>
</author>
<category term="content" />
<category term="recsys" />
<summary>Paper’s contentContent-based recommender systems follow a different approach from the ones we’ve studied so far. They analyze an item’s description to identify others that would be interesting to a certain user. As a system of this kind would need to analyze thousands of items in short periods of time, in order to recommend accurately to a user, every part of the process should be as efficient as possible. Therefore, it is of interest to represent each item in a way that it makes possible to process easily.</summary>
</entry>
<entry>
<title>Week 4 Comments: Collaborative Filtering for Social Tagging Systems: an Experiment with CiteULike</title>
<link href="http://thomashepner.github.io/week-4-comments-collaborative-filtering-for-social-tagging-systems-an-experiment-with-citeulike/" rel="alternate" type="text/html" title="Week 4 Comments: Collaborative Filtering for Social Tagging Systems: an Experiment with CiteULike" />
<published>2016-09-04T16:00:00-03:00</published>
<updated>2016-09-04T16:00:00-03:00</updated>
<id>http://thomashepner.github.io/week-4-comments-collaborative-filtering-for-social-tagging-systems-an-experiment-with-citeulike</id>
<content type="html" xml:base="http://thomashepner.github.io/week-4-comments-collaborative-filtering-for-social-tagging-systems-an-experiment-with-citeulike/">&lt;h2 id=&quot;papers-content&quot;&gt;Paper’s content&lt;/h2&gt;
&lt;p&gt;As user-contributed content is becoming more diverse in nature and quality, and traditional 5-10 point ratings are mostly unavailable, there are some new challenges for collaborative tagging systems. The loss of quality control and fine-grained ratings in these systems can be compensated by tags and explicit connections between users. Still, the paper states that there was no standard in how to take these features into account. There is some evidence, though, that social links improve personalization quality.&lt;/p&gt;

&lt;p&gt;The experiment conducted during this investigation consisted in crawling data from CiteULike: first, crawl a user’s posted articles. Then, calculate neighborhoods of users who posted the same articles and who share the same tags. Finally, use Krovetz’ algorithm to modify tags.&lt;/p&gt;

&lt;p&gt;The algorithm tested consisted of two steps: find the neighborhood of the center user, and rank the articles to be recommended. They used three different approaches: &lt;strong&gt;Classic Collaborative Filtering&lt;/strong&gt;, &lt;strong&gt;Neighbor-weighted CF&lt;/strong&gt;, and &lt;strong&gt;BM25-based Similarity&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;While testing this different methods, they discovered that, as predicted, CCF performed worse than the other algorithms, that where very close in terms of performance. The results also led to find out that the ranking order of the recommendations was very close to the optimal one.&lt;/p&gt;

&lt;p&gt;An important conclusion was that &lt;strong&gt;including the number of raters in the ranking formula is an important factor to consider in the success of these recommendations&lt;/strong&gt;, as it takes into account the weight a good rater has, compared too poor raters of a certain article. In doing so, this practice reduces the uncertainty produced by items with too few ratings.&lt;/p&gt;

&lt;p&gt;As Pearson’s correlation performs badly in this context, tag-based approaches can be a suitable alternative to obtain user neighborhood in social tagging systems.&lt;/p&gt;

&lt;h2 id=&quot;comments-and-critics&quot;&gt;Comments and critics&lt;/h2&gt;
&lt;p&gt;The paper introduces the BM25-based formula for calculating similarity, a non-binary probabilistic method used in information retrieval that returns the relevance that the documents of one collection has given a query. It compares the tags of the center user with a query, and the set of tags of each neighbor as a document. This method was first used in the Okapi BM25 information retrieval system in the 1980s.&lt;/p&gt;

&lt;p&gt;There are several extensions of the algorithm, like BM25F and BM25+. The first one considers the document to be composed from several fields, like headers and main text; the second addresses unfairly scored documents that have a similar relevance to shorter documents that do not contain the search query at all.&lt;/p&gt;

&lt;p&gt;I think the use of information retrieval techniques is a good choice in this kind of systems, especially because tags are a structured form of information, therefore making it easier to retrieve information and rank recommendations.&lt;/p&gt;
</content>
<author>
<name>thomashepner</name>
</author>
<category term="content" />
<category term="recsys" />
<summary>Paper’s contentAs user-contributed content is becoming more diverse in nature and quality, and traditional 5-10 point ratings are mostly unavailable, there are some new challenges for collaborative tagging systems. The loss of quality control and fine-grained ratings in these systems can be compensated by tags and explicit connections between users. Still, the paper states that there was no standard in how to take these features into account. There is some evidence, though, that social links improve personalization quality.</summary>
</entry>
<entry>
<title>Week 3 Comments: Recommender Systems: Sources of Knowledge and Evaluation Metrics</title>
<link href="http://thomashepner.github.io/week-3-comments-recommender-systems-sources-of-knowledge-and-evaluation-metrics/" rel="alternate" type="text/html" title="Week 3 Comments: Recommender Systems: Sources of Knowledge and Evaluation Metrics" />
<published>2016-08-28T15:35:00-03:00</published>
<updated>2016-08-28T15:35:00-03:00</updated>
<id>http://thomashepner.github.io/week-3-comments-recommender-systems-sources-of-knowledge-and-evaluation-metrics</id>
<content type="html" xml:base="http://thomashepner.github.io/week-3-comments-recommender-systems-sources-of-knowledge-and-evaluation-metrics/">&lt;h2 id=&quot;papers-content&quot;&gt;Paper’s content&lt;/h2&gt;
&lt;p&gt;This paper offers an introduction to Recommender Systems, doing a quick review of their history and some of the most popular related algorithms used. First, it shows how these systems are classified in terms of &lt;strong&gt;how they get their input information&lt;/strong&gt;, their learning process (&lt;strong&gt;memory or model based&lt;/strong&gt;), etc. Thus, they fall into three categories: &lt;strong&gt;rule-based&lt;/strong&gt;, &lt;strong&gt;content-based&lt;/strong&gt;, and &lt;strong&gt;usage-based&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Ruler-based systems make decisions based on some rules extracted from user profiles. They’re popular within e-commerce systems, where administrators can set rules based on statistical, psychological and demographic information about users. A problem with this systems are the complex methods used in generating profiles, and biased inputs for the filters.&lt;/p&gt;

&lt;p&gt;Content-based recommender systems provide recommendations to users based on comparing items to others that the user has shown interest in. Thus, the user’s profile becomes an explanation of product characteristics that the user chose before. It relies on information retrieval techniques such as classification, clustering and text analysis.&lt;/p&gt;

&lt;p&gt;Finally, Collaborative Filtering systems are also successful in e-commerce solutions, performing standard memory-based classification based on neighborhoods.&lt;/p&gt;

&lt;p&gt;More importantly, the paper assesses several evaluation metrics for recommender systems. They can be classified into &lt;strong&gt;prediction based metrics&lt;/strong&gt;, that tells which algorithm makes fewer mistakes when inferring how a user will evaluate a proposed recommendation; &lt;strong&gt;information retrieval related metrics&lt;/strong&gt;, used when the user has the option to evaluate an item as relevant or not from a list of recommendations; and &lt;strong&gt;diversity, novelty, and coverage&lt;/strong&gt;, to compare between algorithms.&lt;/p&gt;

&lt;h2 id=&quot;comments-and-critics&quot;&gt;Comments and critics&lt;/h2&gt;
&lt;p&gt;As this paper is more of a review than an investigation, it is an informative way of displaying the state of the art in recommender systems (and as it was released in 2013, it is still an updated source of information).&lt;/p&gt;

&lt;p&gt;I found interesting to learn about the Netflix Prize, and how a contest could drive a whole industry into improving its performance. Researching further, I found out that the secret before team Chaos’ success was combining different algorithms in their own account to improve final performance.&lt;/p&gt;

&lt;p&gt;Several attempts of the team would lead into 7%, 8% improvements for the recommendation algorithm. Surprisingly (and un-intuitively), combining different approaches led to better results, even by continuously introducing new approaches to the solution.&lt;/p&gt;

&lt;p&gt;Some of the team’s ideas were slicing the dataset by frequency, as people who rate a big bunch of movies at once tend to be rating movies that they watched a long time ago, showing different criteria for doing so. They also discovered that people tend to rate differently on Fridays versus Mondays, so moods appear by day of the week.&lt;/p&gt;

&lt;p&gt;This relates to the paper because of the importance of knowing which evaluation metric to use in studying an algorithm. In the Netflix Prize, they used the Root Mean Squared Error metric, giving less importance to coverage, novelty and diversity, and focusing on accuracy.&lt;/p&gt;
</content>
<author>
<name>thomashepner</name>
</author>
<category term="metrics" />
<category term="sources" />
<category term="recsys" />
<summary>Paper’s contentThis paper offers an introduction to Recommender Systems, doing a quick review of their history and some of the most popular related algorithms used. First, it shows how these systems are classified in terms of how they get their input information, their learning process (memory or model based), etc. Thus, they fall into three categories: rule-based, content-based, and usage-based.</summary>
</entry>
<entry>
<title>Week 2 Comments: Slope One Predictors for Online Rating-Based Collaborative Filtering</title>
<link href="http://thomashepner.github.io/week-2-comments-slope-one-predictors-for-online-rating-based-collaborative-filtering/" rel="alternate" type="text/html" title="Week 2 Comments: Slope One Predictors for Online Rating-Based Collaborative Filtering" />
<published>2016-08-21T22:00:00-03:00</published>
<updated>2016-08-21T22:00:00-03:00</updated>
<id>http://thomashepner.github.io/week-2-comments-slope-one-predictors-for-online-rating-based-collaborative-filtering</id>
<content type="html" xml:base="http://thomashepner.github.io/week-2-comments-slope-one-predictors-for-online-rating-based-collaborative-filtering/">&lt;h2 id=&quot;papers-content&quot;&gt;Paper’s content&lt;/h2&gt;
&lt;p&gt;This research shows another model-based approach for making collaborative filtering. The researcher’s main focus is to prove that a new model, called Slope One predictors, could provide:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ease of implementation.&lt;/li&gt;
  &lt;li&gt;Ability to be updateable at execution.&lt;/li&gt;
  &lt;li&gt;Efficiency at query time: queries fast at the expense of storage.&lt;/li&gt;
  &lt;li&gt;Little expectation from first visitors.&lt;/li&gt;
  &lt;li&gt;Accuracy within reason: a minor gain in accuracy is not always worth a major sacrifice in simplicity or scalability.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A concept called &lt;em&gt;popularity differential&lt;/em&gt; is introduced, or how much better is one item liked than another. Many of this differentials exist within a training set for each unknown rating, and Slope One loosely takes advantage of them.&lt;/p&gt;

&lt;p&gt;More specifically, Slope One predictors take into account both information from users who rated the same item, and from the other items rated by the same user - but it also considers data that is not factored in. In doing this, it only takes in ratings by users &lt;strong&gt;who have rated some common item with the &lt;em&gt;predictee&lt;/em&gt; user&lt;/strong&gt;, and only those ratings of items &lt;strong&gt;that the predictee user has also rated&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;But this first approach doesn’t take into account the number of ratings an observer has. As a user who has many ratings would be a much better predictor, the &lt;strong&gt;weighted slope one predictor&lt;/strong&gt; takes it into consideration. A more specific elaboration of the algorithm, called the &lt;strong&gt;bipolar slope one scheme&lt;/strong&gt;, splits the prediction in two parts: one for users that liked and one for users that disliked an item, applying a threshold between user’s liked and disliked items.&lt;/p&gt;

&lt;h2 id=&quot;comments-and-critics&quot;&gt;Comments and critics&lt;/h2&gt;
&lt;p&gt;The first consideration to have while studying Slope One is the one regarding the splitting the prediction into two. This is because this action “doubles” the number of users, but reduces the overall number of ratings in the calculation of the prediction, with an improvement in accuracy. Furthermore, splitting ratings into like and dislike subsets could improve accuracy even more. So, it is important to keep in mind a very quantifiable drawback in performance while separating predictions into groups.&lt;/p&gt;

&lt;p&gt;Another question arises: what happens when the rating scale is not necessarily ordinal, when the difference between a 5-star rating and 4-stars is not the same as the one between 4-stars and 3-stars? It seems that calculating a threshold would be a more complex operation, hence making it harder to improve accuracy.&lt;/p&gt;

&lt;p&gt;Still, this algorithm finally addresses the scalability problem while making the fair assumption that you only use a “cross shaped section” of the user-item matrix. Even though this assumption, I find it smart, given that the sparseness of the matrix is also being taken care of.&lt;/p&gt;

&lt;p&gt;Further diving into recommender system’s history would show that Slope One predictors would become an important improvement, and be rapidly used by several enterprises.&lt;/p&gt;
</content>
<author>
<name>thomashepner</name>
</author>
<category term="collaborative" />
<category term="filtering" />
<category term="recsys" />
<summary>Paper’s contentThis research shows another model-based approach for making collaborative filtering. The researcher’s main focus is to prove that a new model, called Slope One predictors, could provide:</summary>
</entry>
<entry>
<title>Week 2 Comments: Item-based Collaborative Filtering Recommendation Algorithms</title>
<link href="http://thomashepner.github.io/week-2-comments-item-based-collaborative-filtering-recommendation-algorithms/" rel="alternate" type="text/html" title="Week 2 Comments: Item-based Collaborative Filtering Recommendation Algorithms" />
<published>2016-08-21T22:00:00-03:00</published>
<updated>2016-08-21T22:00:00-03:00</updated>
<id>http://thomashepner.github.io/week-2-comments-item-based-collaborative-filtering-recommendation-algorithms</id>
<content type="html" xml:base="http://thomashepner.github.io/week-2-comments-item-based-collaborative-filtering-recommendation-algorithms/">&lt;h2 id=&quot;papers-content&quot;&gt;Paper’s content&lt;/h2&gt;
&lt;p&gt;This paper introduces a brand new approach to Collaborative Filtering. Instead of computing predictions for ratings that a user would give to an item based on user-to-user similarity, this algorithm isolates the users that have rated a given pair of items an then computes the similarity &lt;strong&gt;between those items&lt;/strong&gt;. They also present several ways to calculate this similarity, like &lt;em&gt;cosine based&lt;/em&gt;, &lt;em&gt;correlation based&lt;/em&gt;, and &lt;em&gt;adjusted-cosine&lt;/em&gt; based similarities, and show how to use them to make predictions.&lt;/p&gt;

&lt;p&gt;While trying to make predictions with this approach, the problem of two distant item vector arises: correlation may be misleading when the Euclidian distance between vectors is large, but they are still similar. That’s why they introduce a regression model for weighting similarities, so instead of using raw rating values, approximated values are used based on a linear regression model.&lt;/p&gt;

&lt;p&gt;This research’s focus is on finding out if this different approach to Collaborative Filtering is similar or better than regular Collaborative Filtering, especially in terms of scalability. They discovered that, as item-based recommendation is model-based, it provided higher scalability, as it isolated neighborhood generation and prediction generation steps. This is mainly because the set of items is static compared to the number of users, that changes more often. Still, precomputing all item-to-item similarities would need O(n^2) space, which is not desirable in terms of performance. A solution to this would be retaining only small numbers of similar items (&lt;em&gt;k&lt;/em&gt; sized groups), with an obvious tradeoff in accuracy and coverage.&lt;/p&gt;

&lt;h2 id=&quot;comments-and-critics&quot;&gt;Comments and critics&lt;/h2&gt;
&lt;p&gt;I was skeptical on this new approach’s performance at first. It seemed to me than “inverting the problem’s matrix” would not yield improvements of any kind. But it turns out that the researchers have a fair point in having the strong assumption that the number or items remains fairly static through time, at least compared to the amount of users.&lt;/p&gt;

&lt;p&gt;While trying to understand this approach, I took as an example the CiberDays that several e-commerces organize every now and then. Even though they introduce a large amount of new items every season, user growth escalates way quicker, and the advantages provided by pre-computation of item similarity may give an e-commerce platform the upper hand against its competition while trying to engage more users in better ways and in limited time.&lt;/p&gt;

&lt;p&gt;Although this ways makes more sense (at least to me: when you are in an e-commerce store, you expect to see related items to what you are searching - that’s a fair assumption), it appears that the advantages are not as grand as one would expect, so the added complexity may not be worth the risk.&lt;/p&gt;

&lt;p&gt;Still, this paper shows something that would be very important in the coming years: model-based algorithms provide high online performance, while making fairly strong assumptions. So by knowing the dataset well, you could implement a solution similar to this without loosing recommendation quality.&lt;/p&gt;
</content>
<author>
<name>thomashepner</name>
</author>
<category term="collaborative" />
<category term="filtering" />
<category term="recsys" />
<summary>Paper’s contentThis paper introduces a brand new approach to Collaborative Filtering. Instead of computing predictions for ratings that a user would give to an item based on user-to-user similarity, this algorithm isolates the users that have rated a given pair of items an then computes the similarity between those items. They also present several ways to calculate this similarity, like cosine based, correlation based, and adjusted-cosine based similarities, and show how to use them to make predictions.</summary>
</entry>
</feed>
